{"paper_id": "01I55gys19", "pdf_url": "https://openreview.net/pdf?id=01I55gys19", "markdown_content": "# Few-Class Arena: A Benchmark for Efficient Selection of Vision Models and Dataset Difficulty Measurement\n\nAnonymous Author(s)   \nAffiliation   \nAddress   \nemail\n\n# Abstract\n\n1 A wide variety of benchmark datasets with many classes (80-1000) have been   \n2 created to assist Computer Vision architectural evolution. An increasing number of   \n3 vision models are evaluated with these many-class datasets. However, real-world   \n4 applications often involve substantially fewer classes of interest (2-10). This gap   \n5 between many and few classes makes it difficult to predict performance of the   \n6 few-class applications using models trained on the available many-class datasets.   \n7 To date, little has been offered to evaluate models in this Few-Class Regime. We   \n8 propose Few-Class Arena $( F C A )$ , as a unified benchmark with focus on testing.   \n9 efficient image classification models for few classes. We conduct a systematic   \n10 evaluation of the ResNet family trained on ImageNet subsets from 2 to 1000 classes,   \n11 and test a wide spectrum of Convolutional Neural Networks and Transformer   \n12 architectures over ten datasets by using our newly proposed FCA tool. Furthermore,   \n13 to aid an up-front assessment of dataset difficulty and a more efficient selection   \n14 of models, we incorporate a difficulty measure as a function of class similarity.   \n15 $F C A$ offers a new tool for efficient machine learning in the Few-Class Regime,   \n16 with goals ranging from a new efficient class similarity proposal, to lightweight   \n17 model architecture design, to a new scaling law. FCA is user-friendly and can be   \n18 easily extended to new models and datasets, facilitating future research work. Our   \n19 benchmark is available at https: //github. com/fewclassarena/fca.\n\n# 20 1 Introduction\n\n21The de-facto benchmarks for evaluating efficient vision models are large scale with many classes   \n22 (e.g. 1000 in ImageNet [1], 80 in COCO [2], etc.). Such benchmarks have expedited the advance of   \n23 vision neural networks toward efficiency [3, 4, 5, 6, 7, 8, 9, 10] with the hope of reducing the financial   \n24 and environmental cost of vision models [11, 12]. More efficient computation is facilitated by using   \n25 quantization [13, 14, 15], pruning [16, 17, 18, 19], and data saliency [20]. Despite efficiency   \n26 improvements such as these, many-class datasets are still the standard of model evaluation.   \n27 Real-world applications, however, typically comprise only a few number of classes (e.g, less than   \n28 10) [21, 22, 23] which we termed Few-Class Regime. To deploy a vision model pre-trained on large   \n29 datasets in a specific environment, it requires the re-evaluation of published models or even retraining   \n30 to find an optimal model in an expensive architectural search space [24].   \n31 One major finding is that, apart from scaling down model and architectural design for efficiency,   \n32 dataset difficulty also plays a vital role in model selection [25] (described in Section 4.3).\n\n![](images/6cd710906541476f5b147435f04346dca6cf6832d5806b39a8c1247e553ddfda.jpg)  \n(a) Accuracies for sub-models (blue) and full models (red).\n\n![](images/d676e5cc0b835cd647941b1a29935f49534b6edb2da7e3c1885a58611caad3f6.jpg)  \n(b) Zoomed window shows accuracy values and range for full and sub-models in the few-class range.\n\n(c) Zoomed window shows (c.1) drop of accuracy as $N _ { C L }$ decreases, (c.2) accuracy scales with model size for full models in the few-class range.\n\n![](images/51e1f83ff26bf21462e13198321df9c130a1f50a030a87400b91b9ae5cf3cfd9.jpg)  \n(d) Zoomed window shows (d.1) rising accuracy as $N _ { C L }$ decreases, (d.2) accuracy does not scale with model size for sub-models in the few-class range..\n\n![](images/d08a32a1b365cf0a3231e1fa4c0abd1f8f587930d052b89cda88b6aa82cc12b9.jpg)  \nFigure 1: Top-1 accuracies of various scales of ResNet, whose model sizes are shown in the legend, and whose plots vary from dark to light by decreasing size. Plots range along number of classes $N _ { C l }$ from the full ImageNet size (1000) down to the Few-Class Regime. Each model is tested on 5 subsets whose $N _ { C l }$ classes are randomly sampled from the original 1000 classes. (a) Plots for sub-models trained on subsets of classes (blue) and full models trained on all 1000 classes (red). (b) Zoomed window shows the standard deviation of subset's accuracies is much smaller than for the full model. (c.1) Full model accuracies drop when $N _ { C L }$ decreases. (c.2) Full model accuracies increase as model scales up in the Few-Class Regime. (d.1) Sub-model accuracies grow as $N _ { C L }$ decreases. (d.2) Sub-model accuracies do not increase when model scales up in the Few-Class Regime.\n\n33 Figure 1 summarizes several key findings under the Few-Class Regime. On the left graph in red   \n34 are accuracy results for a range of number of classes $N _ { C L }$ for what we call the \"full model\", that   \n35 is ResNet models pre-trained on the full 1000 classes of ImageNet (generally available from many   \n36 websites). On the right are accuracy results for what we call \"sub-models\", each of which is trained   \n37 and tested on the same $N _ { C L }$ , where this number of classes is sampled from the full dataset down to   \n38 the Few-Class Regime. Findings include the following. (a) Sub-models attain higher upper-bound   \n39 accuracy than full models. (b) The range of accuracy widens for full models at few-classes, which   \n40 increases the uncertainty of a practitioner selecting a model for few classes. In contrast, sub-models   \n41 narrow the range. (c) Full models follow the scaling law [26] in the dimension of model size - larger   \n42 models (darker red) have higher accuracy from many to few classes. (4) Surprisingly, the scaling law   \n43 is violated for sub-models in the Few-Class Regime (see the zoomed-in subplot) where larger models   \n44 (darker blue) do not necessarily perform better than smaller ones (lighter blue). From these plots,   \n45 our key insight is that, instead of using full models, researchers and practitioners in the Few-Class   \n46 Regime should use sub-models for selection of more efficient models.   \n47 However, obtaining sub-models involves computationally expensive training and testing cycles since   \n48 they need to be converged on each of the few-class subsets. By carefully studying and comparing the   \n49 experiment and evaluation setup of these works in the literature, we observe that, how models scale   \n50 down to Few-Class Regime is rarely studied. The lack of comprehensive benchmarks for few-class   \n51 research impedes both researchers and practitioners from quickly finding models that are the most   \n52 efficient for their dataset size. To fill this need, we propose a new benchmark, Few-Class Arena $( F C A )$   \n53 with the goal of benchmarking vision models under few-class scenarios. To our best knowledge, $F C A$   \n54is the first benchmark for such a purpose..   \n55 We formally define Few-Class Regime as a scenario where the dataset has a limited number of classes.   \n56 Real-world applications often comprise only a few number of classes (e.g. $N _ { C L } < 1 0$ or $1 0 \\%$ classes   \n57 of a dataset). Consequently, Few-Class Arena refers to a benchmark to conduct research experiments   \n58 to compare models in the Few-Class Regime. This paper focuses on the image classification task,   \n59 although Few-Class Regime can generalize to object detection and other visual tasks.\n\n60 Statement of Contributions. Four contributions are listed below:\n\nTo be best of our knowledge, we are the first to explore the problems in the Few-Class Regime and develop a benchmark tool Few-Class Arena (FCA) to facilitate scientific research, analysis, and discovery for this range of classes.   \n: We introduce a scalable few-class data loading approach to automatically load images and labels in the Few-Class Regime from the full dataset, avoiding the need to duplicate data points for every additional few-class subset.   \nWe incorporate dataset similarity as an inverse difficulty measurement in Few-Class Arena and propose a novel Silhouette-based similarity score named SimSs. By leveraging the visual feature extraction power of CLIP and DINOv2, we show that SimSS is highly correlated with ResNet performance in the Few-Class Regime with Pearson coefficient scores $\\ge 0 . 8 8$   \n: We conduct extensive experiments that comprise ten models on ten datasets and 2-1000 numbers of classes on ImageNet, totalling 1591 training and testing runs. In-depth analyses on this large body of testing reveal new insights in the Few-Class Regime.\n\n# 74 2 Related Work\n\n75 Visual Datasets and Benchmarks. To advance deep neural network research, a wealth of large-scale   \n76 many-class datasets has been developed for benchmarking visual neural networks over a variety of   \n77 tasks. Typical examples ' include 1000 classes in ImageNet [1] for image classification, and 80 object   \n78 categories in COCO [2] for object detection. Previous benchmarks also extend vision to multimodal   \n79 research such as image-text [27, 28, 29, 30]. While prior works often scale up the number of object   \n80 categories for general purpose comparison, studies [31, 32] raise a concern on whether models trained   \n81 on datasets with such a large number of classes (e.g. ImageNet) can be reliably transferred to real   \n82 world applications often with far fewer classes. A close work to ours is vision backbone comparison   \n83 [33] whose focus is on model architectures. Our perspective differs in a focus on cases with fewer   \n84 number of classes, which often better aligns with real-world scenarios.\n\nDataset Difficulty Measurement. Research has shown the existence of inherent dataset difficulty [32] for classification and other analytic tasks. Efficient measurement methods are proposed to characterize dataset difficulty using Silhouette Score [34], K-means Frechet inception distance [35, 36, 37], and Probe nets [25]. Prior studies have proposed image quality metrics using statistical heuristics, including peak signal-to-noise ratio (PSNR) [38], structural similarity (SsIM) Index [39], and visual information fidelity VIF [40]. A neuroscience-based image diffculty metric [32] is defined as the minimum viewing time related to object solution time (OST) [41]. Another type of difficulty measure method consists of additional procedures such as c-score [42], prediction depth [43], and adversarial robustness [44]. Our work aligns with the line of research [45, 46, 47] involving similarity-based difficulty measurements: similar images are harder to distinguish from each other while dissimilar images are easier. Previous studies are mainly in the image retrieval context [48, 49, 50]. Similarity score is used in [51] with the limitation that a model serving similarity measurement has to be trained for one dataset. We push beyond this limit by leveraging large vision models that learn general visual features using CLIP [52] and DINOv2 [53]. The study [32] shows that CLIP generalizes well to both easy and hard images, making it a good candidate for measuring\n\n100 image difficulty. Supported by the evidence that better classifiers can act as better perceptual feature   \n101 extractors [54], in later sections we show how CLIP and DINOv2 will be used as our similarity base   \n102 function.   \n103 Despite the innovation of difficulty measure algorithms on many-class datasets, little attention has   \n104 been paid to leveraging these methods in the Few-Class Regime. We show that, as the number of   \n105 classes decreases, sub-dataset difficulty in the Few-Class Regime plays a more critical role in efficient   \n106 model selection. To summarize, unlike previous work on many-class benchmarks and difficulty   \n107 measurements, our work takes few-class and similarity-based dataset difficulty into consideration,   \n108 and in doing so we believe the work pioneers the development of visual benchmark dedicated to   \n109 research in the Few-Class Regime.\n\n# o 3 Few-Class Arena (FCA)\n\nWe introduce the Few-Class Arena (FCA) benchmark in this section. In practice, we have integrated FCA into the MMPreTrain framework [55], implemented in Python3 and Pytorch?.\n\n# 3.1 Goals\n\n1. Generality. All vision models and existing datasets for classification should be compatible in this framework. In addition, users can extend to custom models and datasets for their needs.\n\n2. Efficiency. The benchmark should be time- and space-efficient for users. The experimental setup for the few-class benchmark should be easily specified by a few hyper-parameters (e.g. number of classes). Since the few-class regime usually includes sub-datasets extracted from the full dataset, the benchmark should be able to locate those sub-datasets without generating redundant duplicates for reasons of storage efficiency. For time-efficiency, it should conduct training and testing automatically through use of user-specified configuration files, without users' manual execution.\n\n3. Large-Scale Benchmark. The tool should allow for large-scale benchmarking, including training and testing of different vision models on various datasets when the number of classes varies.\n\n# 3.2Few-Class Dataset Preparation\n\nFew-Class Arena provides an easy way to prepare datasets in the Few-Class Regime. By leveraging the MMPreTrain framework, users only need to specify the parameters of few-class subsets in the configuration files, which includes the list of models, datasets, number of classes $( N _ { C L } )$ , and the number of seeds $( N _ { S } )$ . Few-Class Arena generates the specific model and dataset configuration files for each subset, where subset classes are randomly extracted from the full set of classes, as specified by the seed number. Note that only one copy of the full, original dataset is maintained during the whole benchmarking life cycle because few-class subsets are created through the lightweight configurations, thus maximizing storage efficiency. We refer readers to the Appendix and the publicly released link for detailed implementations and use instructions.\n\n# 3.3 Many-Class Full Dataset Trained Benchmark\n\n5 We conducted large-scale experiments spanning ten popular vision models (including CNN and   \n6 ViT architectures) and ten common datasets 3. Except for ImageNet1K, where pre-trained model   \n7 weights are available, we train models in other datasets from scratch. While different models'   \n138 training procedures may incur various levels of complexity (particularly in our case for MobileNet   \n139 V3 and Swin Transformer V2 base), we have endeavored to minimize changes in the existing training   \n140 pipelines from MMPreTrain. The rationale is that if a model exhibits challenges in adapting it to a   \n141 dataset, then it is often not a helpful choice for a practitioner to select for deployment.   \n142 Results are summarized in Table 1. We make several key observations: (1) models in different datasets   \n143 (in rows) yield highly variable levels of performance by Top-1 accuracy; (2) no single best model   \n144 (bold, in columns) exists across all datasets; and (3) model rankings vary across various datasets.   \n145 The first two observations are consistent with the findings in [25, 31]. For (1), it suggests there exists   \n146 underlying dataset-specific difficulty. To capture this characteristic, we adopt the reference dataset   \n147 classification difficulty number (DCN) [25] to refer to the empirically highest accuracy achieved in   \n148 a dataset from a finite number of models shown in Table 1 and Figure 2 (a). For observation (3),   \n149 we can examine the rankings among the ten models of ResNet50 and EfficientNet V2 in Figure 2   \n150 (b). ResNet50's ranking varies dramatically for the different datasets, for instance ranking 7th on   \n151 ImageNet1K and 1st on Quickdraw345. This ranking variability is also observed in other models   \n152 (see all models in the Appendix). However, a common practice is to benchmark models - even for   \n153 efficiency - on large datasets, especially ImageNet1K. The varied dataset rankings in our experiments   \n154 expose the limitations of such a practice, further supporting our new benchmark paradigm, especially   \n155 in the Few-Class Regime. In later sections, we leverage DCN and image similarity for further analysis.\n\n<table><tr><td>Dataset</td><td>RN50 [56]</td><td>VGG16 CNv2 [57]</td><td>[58]</td><td>INCv3 [59]</td><td>EFv2 [4]</td><td>SNv2 [9]</td><td>MNv3 [7]</td><td>ViTb [60]</td><td>[61]</td><td>SWv2b MViTs| [10]</td><td>DCN [25]</td></tr><tr><td>GT43 [62]</td><td>99.85</td><td>96.60</td><td>99.83</td><td>99.78</td><td>99.86</td><td>99.87</td><td>5.98</td><td>99.31</td><td>99.78</td><td>99.69</td><td>99.87</td></tr><tr><td>CF100 [63]</td><td>74.56</td><td>71.12</td><td>85.89</td><td>75.97</td><td>77.05</td><td>77.89</td><td>1.00</td><td>32.65</td><td>78.49</td><td>76.51</td><td>85.89</td></tr><tr><td>IN1K [1]</td><td>76.55</td><td>71.62</td><td>84.87</td><td>77.57</td><td>85.01</td><td>69.55</td><td>67.66</td><td>82.37</td><td>84.6</td><td>78.25</td><td>85.01</td></tr><tr><td>FD101 [64]</td><td>83.76</td><td>75.82</td><td>63.80</td><td>83.96</td><td>80.82</td><td>79.36</td><td>0.99</td><td>52.21</td><td>84.30</td><td>82.23</td><td>84.30</td></tr><tr><td>CT101 [65]</td><td>77.70</td><td>74.99</td><td>77.52</td><td>77.52</td><td>77.82</td><td>84.13</td><td>76.58</td><td>59.59</td><td>78.82</td><td>80.06</td><td>84.13</td></tr><tr><td>CT256 [66]</td><td>65.07</td><td>59.08</td><td>73.57</td><td>66.09</td><td>62.80</td><td>68.13</td><td>22.63</td><td>44.23</td><td>67.28</td><td>65.80</td><td>73.57</td></tr><tr><td>QD345 [67]</td><td>69.14</td><td>19.86</td><td>62.86</td><td>68.25</td><td>68.81</td><td>67.32</td><td>0.72</td><td>19.67</td><td>66.54</td><td>68.76</td><td>69.14</td></tr><tr><td>CB200 [68]</td><td>45.86</td><td>21.26</td><td>27.61</td><td>45.58</td><td>44.48</td><td>53.95</td><td>47.22</td><td>23.73</td><td>54.52</td><td>58.46</td><td>58.46</td></tr><tr><td>ID67 [69]</td><td>53.75</td><td>26.01</td><td>33.21</td><td>45.95</td><td>43.85</td><td>54.72</td><td>49.10</td><td>30.51</td><td>48.58</td><td>54.05</td><td>54.72</td></tr><tr><td>TT47 [70]</td><td>30.43</td><td>12.55</td><td>6.49</td><td>14.20</td><td>21.17</td><td>43.83</td><td>2.18</td><td>31.38</td><td>33.94</td><td>24.41</td><td>43.83</td></tr></table>\n\nTable 1: Top-1 accuracy across ten models in ten datasets. Models are trained and tested on full datasets with their original number of classes (e.g. 1K from ImageNet1K); this is denoted in the last few digits of the abbreviation of the dataset name. The best score is highlighted in bold while the second best is underlined for each dataset.\n\n(a) Top-1 accuracy and DCN in ten full datasets.\n\n![](images/f81c639d38c8182e9219a76f9a57a377bbc45526162b8211abdff4885ec83078.jpg)  \nFigure 2: Many-Class Full Dataset Benchmark.\n\n![](images/6dc868ac8d2871624baac46cdf0edaca745d4e9aac751c5142aef3fd6c8d76d2.jpg)  \n(b) Ranking of ResNet50 (RN50) and EfficientNet V2 (EFv2) across 10 datasets by Top-1 acc.\n\n156 In the next subsections, we introduce three new types of benchmarks: (1) Few-Class, Full Dataset   \n157 Trained Benchmark (FC-Full), which benchmarks vision models trained on the full dataset with the   \n158 original number of classes; (2) Few-Class, Subset Trained Benchmark (FC-Sub), which benchmarks   \n159 vision models trained on subsets of a fewer number of classes than the full dataset, and (3) Few-Class   \n160 Similarity Benchmark (FC-Sim), which benchmarks image similarity methods and their correlation   \n161 with model performance.\n\nTraditionally, a large number of models are trained and compared on many-class datasets. However, results for such benchmarks are not directly useful to the Few-Class Regime and many real-world scenarios. Therefore, we introduce the Few-Class Full Dataset Trained Benchmark (FC-Full), with the objective of effortlessly conducting large-scale experiments and analyses in the Few-Class Regime.\n\n167 The procedure of FC-Full consists of two main stages. In the first stage, users select the models   \n168 and datasets upon which they would like to conduct experiments. They can choose to download   \n169 pre-trained model weights, which are usually available on popular model hubs (PyTorch Hub [71],   \n170 TensorFlow Hub [72], Hugging Face [73], MMPreTrain [55] etc.). In case of no pre-trained weights   \n171 available from public websites, users can resort to the option of training from scratch. To that end,   \n172 our tool is designed and implemented to generate bash scripts for easily configurable and modifiable   \n173 training through the use of configuration files.   \n174 In the second stage, users conduct benchmarking in the Few-Class Regime. By specifying the list of   \n175 classes, Few-Class Arena automatically loads pre-trained weights of the chosen models and evaluates   \n176 performance of the models on the selected datasets. Note that this process is accomplished through   \n177 configuration files created by the user's specifications, thus enabling hundreds of experiments to be   \n178 launched by a single command. This dramatically reduces human effort that would otherwise be   \n179 expended to run these experiments without Few-Class Arena.\n\n# 3.5Few-Class Subset Trained Benchmark (FC-Sub)\n\n31 Our study in Figure 1 (red lines) reveals the limits of existing pre-trained models in the Few-Class   \n32 Regime. To facilitate further research and analyze the upper bound performance in the Few-Class   \n33 Regime, we introduce the Few-Class Subset Trained Benchmark (FC-Sub).\n\nFC-Sub follows a similar procedure to FC-Full, except that, when evaluating a model in a subset with a specific number of classes, that model should have been trained on that same subset. Specifically, in Stage One (described for FC-Full, users specify models, datasets and the list of number of classes in configuration files. Then Few-Class Arena generates bash scripts for model training on each subset. In Stage two, Few-Class Arena tests each model in the same subset that it was trained on.\n\n# 3.6Few-Class Similarity Benchmark (FC-Sim)\n\nOne objective of our tool is to provide the Similarity Benchmark as a platform for researchers to design custom similarity scores for efficient comparison of models and datasets.\n\nThe intrinsic image difficulty of a dataset affects a model's classification performance (and human) [74, 75, 32]. We show - as is intuitive - that the more similar two images are, the more difficult it is for a vision classifier to make a correct prediction. This suggests that the level of similarity of images in a dataset can be used as a proxy for a dataset difficulty measure. In this section, we first adopt and provide the basic formulation of similarity, the baseline of a similarity metric. Then we propose a Similarity-Based Silhouette Score to capture the characteristic of image similarity in a dataset.\n\n198 We first adopt the basic similarity formulation from [51]. Intra-Class Similarity ${ S _ { \\alpha } ^ { ( C ) } }$ is defined as a   \n199 scalar describing the similarity of images within a class by taking the average of all the distinct class   \n200 pairs in $C$ , while Inter-Class Similarity denotes a scalar describing the similarity among images in   \n201 two different classes $C _ { 1 }$ and $C _ { 2 }$ . For a dataset $D$ , these are defined as the mean of their similarity   \n202 scores over all classes, respectively:\n\n$$\nS _ { \\alpha } ^ { ( D ) } = \\frac { 1 } { | L | } \\sum _ { l \\in L } S _ { \\alpha } ^ { ( C _ { l } ) } = \\frac { 1 } { | L | \\times | P ^ { ( C _ { l } ) } | } \\sum _ { l \\in L } \\sum _ { i , j \\in C _ { l } ; } \\cos ( \\mathbf { Z } _ { i } , \\mathbf { Z } _ { j } ) ,\n$$\n\n$$\nS _ { \\beta } ^ { ( D ) } = \\frac { 1 } { | P ^ { ( D ) } | } \\sum _ { a , b \\in L ; a \\neq b } S _ { \\beta } ^ { ( C _ { a } , C _ { b } ) } = \\frac { 1 } { | P ^ { ( D ) } | \\times | P ^ { ( C _ { 1 } , C _ { 2 } ) } | } \\sum _ { a , b \\in L ; a \\neq b } \\sum _ { i \\in C _ { 1 } , j \\in C _ { 2 } } \\cos ( \\mathbf { Z } _ { i } , \\mathbf { Z } _ { j } ) .\n$$\n\n:04 where $| L |$ is the number of classes in a dataset, $Z _ { i }$ is the visual feature of an image. $i$ $| P ^ { ( C ) } |$ is the :05 total number of distinct image pairs in class $C$ $| P ^ { ( D ) } |$ is the total number of distinct class pairs, and :06 $\\left| P ^ { ( C _ { 1 } , C _ { 2 } ) } \\right|$ is the total number of distinct image pairs excluding same-class pairs.\n\nAveraging these similarities provides a single scalar score at the class or dataset level. However, 3 this simplicity neglects other cluster-related information that can better reveal the underlying dataset ) difficulty property of a dataset. In particular, the (1) tightness of a class cluster and (2) distance to ) other classes of class clusters, are features that characterize the inherent class difficulty, but are not captured by $S _ { \\alpha }$ or $S _ { \\beta }$ alone.\n\n212 To compensate the aforementioned drawback, we adopt the Silhouette Score (Ss) [34, 76]: $S S ( i ) =$   \n213 max(-g. where ()is the ilhouette Scor of the data ponti, (i) is the average dissimilarit   \n214 between $i$ and other instances in the same class, and $b ( i )$ is the average dissimilarity between $i$ and   \n215 other data points in the closest different class.   \n216 Observe that the above Intra-Class Similarity $S _ { \\alpha } ^ { ( C ) }$ already represents the tightness of the class $( C )$   \n217 therefore $a ( i )$ can be replaced with the inverse of Intra-Class Similarity $a ( i ) = - S _ { \\alpha } ( i )$ . For the   \n21e second term $b ( i )$ we adopt the preiously deined Intr Clss Similarity $S _ { \\beta } ^ { ( C _ { 1 } , C _ { 2 } ) }$ and introduce a new   \n219 similarity score as Nearest Inter-Class Similarity $S _ { \\beta } ^ { \\prime } { } ^ { ( C ) }$ , which is a scalar describing the similarity   \n220 among instances between class $C$ and the closest class of each instance in. $C$ . The dataset-level   \n221 Nearest Iner-Cass imilrit sis expesed is\n\n$$\n\\boldsymbol { S } _ { \\beta } ^ { \\prime ( D ) } = \\frac { 1 } { | L | } \\sum _ { l \\in L } \\boldsymbol { S } _ { \\beta } ^ { \\prime ( C _ { l } , \\hat { C } _ { l } ) } = \\frac { 1 } { | L | \\times | P ^ { ( C _ { l } , \\hat { C } _ { l } ) } | } \\sum _ { l \\in L } \\sum _ { i \\in C _ { l } , j \\in \\hat { C } _ { l } } \\cos ( \\mathbf { Z } _ { i } , \\mathbf { Z } _ { j } ) .\n$$\n\n222 where $\\hat { C }$ is the set of the nearest class to. $C$ $( { \\hat { C } } \\neq C )$ . To summarize, we introduce our novel   \n223 Similarity-Based Silhouette Score $S i m S S ^ { 4 }$ ..\n\n$$\nS i m S S ^ { ( D ) } = \\frac { 1 } { | L | \\times | C _ { l } | } \\sum _ { i \\in C _ { l } } \\frac { S _ { \\alpha } ( i ) - S ^ { \\prime } { } _ { \\beta } ( i ) } { m a x ( S _ { \\alpha } ( i ) , S ^ { \\prime } { } _ { \\beta } ( i ) ) } .\n$$\n\n# 24 4Experimental Results\n\n# 4.1Results on FC-Full\n\nIn this section, we present the results of FC-Full. A model trained on the dataset with its original number of classes (e.g. 1000 in ImageNet1K) is referred to as a full-class model. These experiments are designed to understand how full-class model performance changes when the number of classes $N _ { C l }$ decreases from many to few classes. We analyze the results of DCN-Full, shown in Figure 3 (details of all models are presented in the Appendix), and we make two key observations when $N _ { C l }$ reduces to the Few-Class Regime (from right to left). (1) The best performing models do not always increase its accuracy for fewer classes, as shown by the solid red lines that represent the average of DCN for each $N _ { C l }$ . (2) The variance, depicted by the light red areas, of the best models broaden dramatically for low $N _ { C l }$ , especially for $N _ { C l } < 1 0$\n\nBoth observations support evidence of the limitations of using the common many-class benchmark for application model selection in the Few-Class Regime, since it is not consistent between datasets that a model can be made smaller with higher accuracy. Furthermore, the large variance in accuracy means that prediction of performance for few classes is unreliable for this approach.\n\n# 4.2Results on FC-Sub\n\n)In this section, we show how using Few-Class Arena can help reveal more insights in the Few-Class I Regime to mitigate the issues of Section 4.1.\n\n![](images/487f1d72e555455b45315f4c2ff8bbe9d3d9ecbf6487ce82b534e84fb3bea925.jpg)  \nFigure 3: DCN-Full by Top-1 Accuracy $( \\% )$ $N _ { C l }$ ranges from many to 2.\n\n242 FC-Sub results are displayed in Figure 4. Recall that a sub-class model is a model trained on a subset   \n243 of the dataset where $N _ { C l }$ is smaller than the original number of classes in the full dataset. Observe   \n244 that in the Few-Class Regime (when $N _ { C l }$ decreases from 4 to 2) that: (1) DCN increases as shown by   \n245 the solid blue lines, and (2) variance reduces as displayed by the light blue areas.   \n246 The preceding observation for FC-Full 4.1 seems to contradict the common belief that, the fewer the   \n247 classes, the higher is the accuracy that a model can achieve. Conversely, the FC-Sub results do align   \n248 with this belief. We argue that a full-class model needs to accommodate many parameters to learn   \n249 features that will enable high performance across all classes in a many-class, full dataset. With the   \n250 same parameters, however, a sub-class model can adapt to finer and more discriminative features that   \n251 improve its performance when the number of target classes are much smaller.\n\n![](images/8e6ae81509c6f205711bda4a696291771839e4df93180119969676e9785129b3.jpg)  \nFigure 4: DCN-Sub (red) and DCN-Full (blue) by Top-1 Accuracy $( \\% )$ $N _ { C L }$ ranges from 2 to 4.\n\n# 252 4.3 Results on FC-Sim\n\n253 In this section, we analyze the use of $\\mathrm { S i m S S }$ (Equation 4) as proxy for few-class dataset difficulty.   \n254 Experiments are conducted on ImageNet1K using the ResNet family for the lower $N _ { C L } \\leq 1 0 \\%$ range   \n255 of the original 1000 classes, $N _ { C L } \\in \\{ 2 , 3 , 4 , 5 , 1 0 , 1 0 0 \\}$ , and the results are shown in Figure 5. Each   \n256 datapoint of DCN-Full (diamond in red) or DCN-Sub (square in blue) represents an experiment in a\n\n57 subset of a specific $N _ { C L }$ , where classes are sampled from the full dataset. For reproducible results, i8 we use seed numbers from 0 to 4 to generate 5 subsets for one $N _ { C L }$ by default. A similarity base 59 function $( s i m ( ) )$ is defined as the atomic function that takes a pair of images as input and outputs a i0 scalar that represents their image similarity..\n\nIn our experiments, we leverage the general visual feature extraction ability of CLIP (image $^ +$ text) [52] and DINOv2 (image) [53] by self-supervised learning. Specifically, a pair of images are fed into its latent space from which the the cosine score is calculated and normalized to 0 to 1. Note that we only use the Image Encoder in CLIP.\n\nComparing Accuracy and Similarity To evaluate $\\mathrm { S i m S S }$ , we compute the Pearson correlation coefficient (PCC) $( r )$ between model accuracy and SimSS. Results in Figure 5 (a) (b) show that $\\mathrm { S i m } { \\bf S } { \\bf S }$ is poorly correlated with DCN-Full ( $r = 0 . 1 8$ and $r = 0 . 2 6$ for CLIP and DINOv2) due to the large variance shown in Section 4.1. In contrast, SimSS is highly correlated with DCN-Sub (shown in blue squares), with $r = 0 . 9 0$ and $r = 0 . 8 8$ using CLIP (dashed) and DINOv2 (solid), respectively. The high PCC [77, 78] demonstrates that $\\mathrm { S i m S S }$ is a reliable metric to estimate few-class dataset difficulty, and this can help predict the empirical upper-bound accuracy of a model in the Few-Class Regime. Comparison between SimSS and all models can be found in the Appendix. Such a high correlation suggests this offers a reliable scaling relationship to estimate model accuracy by similarity for other values of $N _ { C L }$ without an exhaustive search. Due to the dataset specificity of the dataset difficulty property, this score is computed once and used for all times the same dataset is used. We have made available difficulty scores for many datasets at the Few-Class Arena site.\n\n![](images/0c203a06bb7a577ed81fc86acd4c391598e77b808f88fc34a78a80be550f44c4.jpg)  \nFigure 5: Pearson correlation coefficient $( r )$ between DCN and $\\mathrm { { s i m s s } }$ when $N _ { C l } \\in \\{ 2 , 3 , 4 , 5 , 1 0 , 1 0 0 \\}$ . DCNSub (blue squares) is more highly correlated than DCN-Full(red diamonds) with SimSS using both similarity base functions of CLIP (dashed line) and DINOv2 (solid line) with $r \\geq 0 . 8 8$\n\n# 277 5 Conclusion\n\nWe have proposed Few-Class Arena and a dataset difficulty measurement, which together form a benchmark tool to compare and select efficient models in the Few-Class Regime. Extensive experiments and analyses over 1500 experiments with 10 models on 10 datasets have helped identify new behavior that is specific to the Few-Class Regime as compared to for many-classes. One finding reveals a new $n _ { C l }$ -scaling law whereby dataset difficulty must be taken into consideration for accuracy prediction. Such a benchmark will be valuable to the community by providing both researchers and practitioners with a unified framework for future research and real applications.\n\n85 Limitations and Future Work. We note that the convergence of sub-models is contingent on various   \n86 factors in a training scheduler, such as learning rate. A careful tuning of training procedure may   \n87 increase a model's performance, but it shouldn't change the classification difficulty number drastically   \n88 since this represents a dataset's intrinsic difficulty property. The current difficulty benchmark supports   \n89 image similarity while in the future it can be expanded to other difficulty measurements [25]. As   \n90 CLIP and DINOv2 are trained toward general visual features, it is unclear if they will be appropriate   \n91 for other types of images such as sketches without textures in Quickdraw [67] . For this reason, a   \n92 universal similarity foundation model would be appealing that applies to any image type. In summary,   \n93 Few-Class Arena identifies a promising new path to achieve efficiencies that are focused on the   \n94 important and practical Few-Class Regime, establishing this as a baseline for future work.\n\nReferences   \n[1] Deng, J., W. Dong, R. Socher, et al. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009.   \n[2] Lin, T.-Y., M. Maire, S. Belongie, et al. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755. Springer, 2014.   \n[3] Tan, M., Q. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pages 6105-6114. PMLR, 2019.   \n[4] Tan, M., Q. L. Efficientnetv2: Smaller models and faster training. In International conference on machine learning, pages 10096-10106. PMLR, 2021.   \n[5] Sinha, D., M. El-Sharkawy. Thin mobilenet: An enhanced mobilenet architecture. In 2019 IEEE 1Oth annual ubiquitous computing, electronics & mobile communication conference (UEMCON), pages 0280-0285. IEEE, 2019.   \n[6] Sandler, M., A. Howard, M. Zhu, et al. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4510-4520. 2018.   \n[7] Howard, A., M. Sandler, G. Chu, et al. Searching for mobilenetv3. In Proceedings of the 1EEE/CVF international conference on computer vision, pages 1314-1324. 2019.   \n[8] Iandola, F. N., S. Han, M. W. Moskewicz, et al. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and $< 0 . 5 \\mathrm { m b }$ model size. arXiv preprint arXiv:1602.07360, 2016.   \n[9] Ma, N., X. Zhang, H.-T. Zheng, et al. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In Proceedings of the European conference on computer vision (ECCV), pages 116-131. 2018.   \n[10] Mehta, S., M. Rastegari. Mobilevit: Light-weight, general-purpose, and mobile-friendly vision transformer. arxiv 2021. arXiv preprint arXiv:2110.02178.   \n[11] Patterson, D., J. Gonzalez, Q. Le, et al. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350, 2021.   \n[12] Rae, J. W., S. Borgeaud, T. Cai, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.   \n[13] Gysel, P., J. Pimentel, M. Motamedi, et al. Ristretto: A framework for empirical study of resource-efficient inference in convolutional neural networks. IEEE transactions on neural networks and learning systems, 29(11):5784-5789, 2018.   \n[14] Han, S., H. Mao, W. J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.   \n[15] Leng, C., Z. Dou, H. Li, et al. Extremely low bit neural network: Squeeze the last bit out with admm. In Proceedings of the AAAI conference on artificial intelligence, vol. 32. 2018.   \n[16] Cheng, Y., D. Wang, P. Zhou, et al. A survey of model compression and acceleration for deep neural networks. arXiv preprint arXiv:1710.09282, 2017.   \n[17] Blalock, D., J. J. Gonzalez Ortiz, J. Frankle, et al. What is the state of neural network pruning? Proceedings of machine learning and systems, 2:129-146, 2020.   \n[18] Li, H., A. Kadav, I. Durdanovic, et al. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016.   \n[19] Shen, M., H. Yin, P. Molchanov, et al. Structural pruning via latency-saliency knapsack. arXiv preprint arXiv:2210.06659, 2022.   \n[20] Yeung, S., O. Russakovsky, G. Mori, et al. End-to-end learning of action detection from frame glimpses in videos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2678-2687. 2016.   \n[21] Shao, Z., L. Wang, Z. Wang, et al. Saliency-aware convolution neural network for ship detection in surveillance video. IEEE Transactions on Circuits and Systems for Video Technology, 30(3):781-794, 2020.   \n[22] A. Delplanque, P. L. J. L. J. T., S. Foucher. Multispecies detection and identification of african mammals in aerial imagery using convolutional neural networks. Remote Sensing in Ecology and Conservation, 8(April):166-179, 2022.   \n[23] Cai, Y., T. Luan, H. Gao, et al. Yolov4-5d: An effective and efficient object detector for autonomous driving. IEEE Transactions on Instrumentation and Measurement, 70:1-13, 2021.   \n[24] Scheidegger, F., L. Benini, C. Bekas, et al. Constrained deep neural network architecture search for iot devices accounting for hardware calibration. Advances in Neural Information Processing Systems, 32, 2019.   \n[25] Scheidegger, F., R. Istrate, G. Mariani, et al. Efficient image dataset classification difficulty estimation for predicting deep-learning accuracy. The Visual Computer, 37(6): 1593-1610, 2021.   \n[26] Kaplan, J., S. McCandlish, T. Henighan, et al. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.   \n[27] Lee, T., M. Yasunaga, C. Meng, et al. Holistic evaluation of text-to-image models. Advances in Neural Information Processing Systems, 36, 2024.   \n[28] Le, T., V. Lal, P. Howard. Coco-counterfactuals: Automatically constructed counterfactual examples for image-text pairs. Advances in Neural Information Processing Systems, 36, 2024.   \n[29] Laurengon, H., L. Saulnier, L. Tronchon, et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. Advances in Neural Information Processing Systems, 36, 2024.   \n[30] Bitton, Y., N. Bitton Guetta, R. Yosef, et al. Winogavil: Gamified association benchmark to challenge vision-and-language models. Advances in Neural Information Processing Systems, 35:26549-26564, 2022.   \n[31] Fang, A., S. Kornblith, L. Schmidt. Does progress on imagenet transfer to real-world datasets? Advances in Neural Information Processing Systems, 36, 2024.   \n[32] Mayo, D., J. Cummings, X. Lin, et al. How hard are computer vision datasets? calibrating dataset difficulty to viewing time. Advances in Neural Information Processing Systems, 36:11008 11036, 2023.   \n[33] Goldblum, M., H. Souri, R. Ni, et al. Battle of the backbones: A large-scale comparison of pretrained models across computer vision tasks. Advances in Neural Information Processing Systems, 36, 2024.   \n[34] Rousseeuw, P. J. Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. Journal of computational and applied mathematics, 20:53-65, 1987.   \n[35] Dowson, D., B. Landau. The frechet distance between multivariate normal distributions. Journal of multivariate analysis, 12(3):450-455, 1982.   \n[36] Heusel, M., H. Ramsauer, T. Unterthiner, et al. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.   \n[37] Lucic, M., K. Kurach, M. Michalski, et al. Are gans created equal? a large-scale study. Advances in neural information processing systems, 31, 2018.   \n[38] Hore, A., D. Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th international conference on pattern recognition, pages 2366-2369. IEEE, 2010.   \n[39] Wang, Z., A. C. Bovik, H. R. Sheikh, et al. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600-612, 2004.   \n[40] Sheikh, H. R., A. C. Bovik. Image information and visual quality. IEEE Transactions on image processing, 15(2):430-444, 2006.   \n[41] Kar, K., J. Kubilius, K. Schmidt, et al. Evidence that recurrent circuits are critical to the ventral stream's execution of core object recognition behavior. Nature neuroscience, 22(6):974-983, 2019.   \n[42] Jiang, Z., C. Zhang, K. Talwar, et al. Characterizing structural regularities of labeled data in overparameterized models. arXiv preprint arXiv:2002.03206, 2020.   \n[43] Baldock, R., H. Maennel, B. Neyshabur. Deep learning through the lens of example difficulty. Advances in Neural Information Processing Systems, 34:10876-10889, 2021.   \n[44] Goodfellow, I. J., J. Shlens, C. Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.   \n[45] Arun, S. Turning visual search time on its head. Vision Research, 74:86-92, 2012.   \n[46] Trick, L. M., J. T. Enns. Lifespan changes in attention: The visual search task. Cognitive Development, 13(3):369-386, 1998.   \n[47] Wolfe, J. M., E. M. Palmer, T. S. Horowitz. Reaction time distributions constrain models of visual search. Vision research, 50(14):1304-1311, 2010.   \n[48] Zhang, D., G. Lu. Evaluation of similarity measurement for image retrieval. In International conference on neural networks and signal processing, 2003. proceedings of the 2003, vol. 2, pages 928-931. IEEE, 2003.   \n[49] Wang, J., Y. Song, T. Leung, et al. Learning fine-grained image similarity with deep ranking. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1386-1393. 2014.   \n[50] Tudor Ionescu, R., B. Alexe, M. Leordeanu, et al. How hard can it be? estimating the difficulty of visual search in an image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2157-2166. 2016.   \n[51] Cao, B. B., L. O'Gorman, M. Coss, et al. Data-side efficiencies for lightweight convolutional neural networks. arXiv preprint arXiv:2308.13057, 2023.   \n[52] Radford, A., J. W. Kim, C. Hallacy, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.   \n[53] Oquab, M., T. Darcet, T. Moutakanni, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.   \n[54] Kumar, M., N. Houlsby, N. Kalchbrenner, et al. Do better imagenet classifiers assess perceptual similarity better? arXiv preprint arXiv:2203.04946, 2022.   \n[55] Contributors, M. Openmmlab's pre-training toolbox and benchmark. https : //github. com/ open-mmlab/mmpretrain, 2023.   \n[56] He, K., X. Zhang, S. Ren, et al. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778. 2016.   \n[57] Simonyan, K., A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.   \n[58] Woo, S., S. Debnath, R. Hu, et al. Convnext v2: Co-designing and scaling convnets with masked autoencoders. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16133-16142. 2023.   \n[59] Szegedy, C., V. Vanhoucke, S. Ioffe, et al. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818-2826. 2016.   \n[60] Dosovitskiy, A., L. Beyer, A. Kolesnikov, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\n435 [61] Liu, Z., H. Hu, Y. Lin, et al. Swin transformer v2: Scaling up capacity and resolution. In   \n436 Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages   \n437 12009-12019. 2022.   \n438 [62] Stallkamp, J., M. Schlipsing, J. Salmen, et al. Man vs. computer: Benchmarking machine   \n439 learning algorithms for traffic sign recognition. Neural Networks, (0):-, 2012.   \n440 [63] Krizhevsky, A., G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n441 [64] Bossard, L., M. Guillaumin, L. Van Gool. Food-101 - mining discriminative components with   \n442 random forests. In European Conference on Computer Vision. 2014.   \n443 [65] Li, F.-F., M. Andreeto, M. Ranzato, et al. Caltech 101, 2022.   \n444 [66] Griffin, G., A. Holub, P. Perona. Caltech 256, 2022.   \n445 [67] Ha, D., D. Eck. A neural representation of sketch drawings. arXiv preprint arXiv:1704.03477,   \n446 2017.   \n447 [68] Wah, C., S. Branson, P. Welinder, et al. The Caltech-UCsD Birds-200-201l Dataset. 2011.   \n448 [69] Quattoni, A., A. Torralba. Recognizing indoor scenes. In 2009 IEEE conference on computer   \n449 vision and pattern recognition, pages 413-420. IEEE, 2009.   \n450 [70] Cimpoi, M., S. Maji, I. Kokkinos, et al. Describing textures in the wild. In Proceedings of the   \n451 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2014.   \n452 [71] Foundation, T. L. Pytorch hub. https://pytorch. org/hub, 2024. Accessed on 2024-06-04.   \n453 [72] Inc., G. Tensorflow hub. https://www.tensorf1ow. org/hub, 2024. Accessed on 2024-06-   \n454 04.   \n455 [73] Face, H. Hugging face models. https://huggingface.co/models, 2024. Accessed on   \n456 2024-06-04.   \n457 [74] Geirhos, R., D. H. Janssen, H. H. Sch\"utt, et al. Comparing deep neural networks against   \n458 humans: object recognition when the signal gets weaker. arXiv preprint arXiv:1706.06969,   \n459 2017.   \n460 [75] Rajalingham, R., E. B. Issa, P. Bashivan, et al. Large-scale, high-resolution comparison of the   \n461 core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial   \n462 neural networks. Journal of Neuroscience, 38(33):7255-7269, 2018.   \n463 [76] Shahapure, K. R., C. Nicholas. Cluster quality analysis using silhouette score. In 2020 IEEE   \n464 7th international conference on data science and advanced analytics (DSAA), pages 747-748.   \n465 IEEE, 2020.   \n466 [77] Wicklin, R. Weak or strong? how to interpret a spearman or kendall correlation. https:   \n467 //blogs.sas.com/content/im1/2023/04/05/interpret-spearman-kendal1-corr.   \n468 htm1, 2024. Accessed on 2024-06-04.   \n469 [78] Schober, P., C. Boer, L. A. Schwarte. Correlation coefficients: appropriate use and interpretation.   \n470 Anesthesia & analgesia, 126(5):1763-1768, 2018.", "status": "success", "markdown_file": "full.md"}
{"paper_id": "08nbMTxazb", "pdf_url": "https://openreview.net/pdf?id=08nbMTxazb", "markdown_content": "# Chicks4FreeID: A Benchmark Dataset for Chicken Re-Identification\n\nDaria Kern1,2 $^ *$ Tobias Schiele1,2 $^ *$ Ulrich Klauck1,3 Winfred Ingabire2\n\n1 Aalen University, Germany {daria.kern, tobias.schiele, ulrich.klauck}@hs-aalen.de 2Glasgow Caledonian University, United Kingdom. winfred.ingabire@gcu.ac.uk 3University of the Western Cape, South Africa\n\n![](images/aceb05ed69fca302c79e062eb94f2dddbba50578a24412b4677eddaebe4395c9.jpg)  \nFigure 1: Excerpt from the Chicks4FreeID dataset.\n\n# Abstract\n\n1 To address the need for well-annotated datasets in the field of animal re  \n2 identification, and particularly to close the existing gap for chickens, we intro  \n3 duce the Chicks4FreeID dataset. This dataset is the first publicly available re  \n4 identification resource dedicated to the most farmed animal in the world. It in  \n5 cludes top-down view images of individually segmented and annotated chickens,   \n6 along with preprocessed cut-out crops of the instances. The dataset comprises   \n7 1215 annotations of 50 unique chicken individuals, as well as a total of 55 an  \n8 notations of 2 roosters and 2 ducks. In addition to re-identification, the dataset   \n9 supports semantic and instance segmentation tasks by providing corresponding   \n10 masks. Curation and annotation were performed manually, ensuring high-quality,   \n11 nearly pixel-perfect masks and accurate ground truth assignment of the individuals   \n12 using expert knowledge. Additionally, we provide context by offering a compre  \n13 hensive overview of existing datasets for animal re-identification. To facilitate   \n14 comparability, we establish a baseline for the re-identification task testing dif  \n15 ferent approaches. Performance is evaluated based on mAP, Top-1, and Top-5   \n16 accuracy metrics. Both the data and code are publicly shared under a CC BY   \n17 4.0 license, promoting accessibility and further research. The dataset can be ac  \n18 cessed at https://huggingface.co/datasets/dariakern/Chicks4FreeID and the code at   \n19 https://github.com/DariaKern/Chicks4FreeID.\n\n# 20 1 Introduction\n\n# 11.1  Motivation\n\nChickens struggle to recognize other individuals after visible changes are applied to the comb or 3 plumage [1]. Much like chickens are able to use visual cues to differentiate each other, artificial intelligence (AI) is capable of utilizing image or video inputs for re-identification purposes. AI-driven ; re-identification and tracking systems hold great potential for enhancing animal husbandry and ; livestock farming. These systems may allow for the observation of social structures and behavior, enhance welfare, and potentially lead to more efficient animal management with minimal disruption 3 to the livestock [2]. They also may help assess health and well-being, i.e., by providing crucial ) traceability during disease outbreaks. Furthermore, they offer a cost-effective and non-invasive ) alternative to manual tagging methods.\n\n31 Despite the significant potential, there is a notable gap in publicly available datasets for such   \n32 technologies, especially for chickens - the most farmed animal globally. Remarkably, to our   \n33 knowledge, no publicly available dataset for chicken re-identification exists, highlighting an urgent   \n34 need for development in this field. Public datasets for the task of individual animal re-identification   \n35 in general are scarce [3, 2]. In particular, well-annotated datasets [4]. The practice of openly sharing   \n36 data and code should be encouraged to enhance result comparability, yet not all research data are   \n37 currently made public. In their work, [5] emphasize the importance of creating and sharing publicly   \n38 available and well-annotated benchmark datasets for the task of animal re-identification.   \n39 Establishing a benchmark dataset involves evaluating how well existing methods solve the dataset.   \n40 The reported metrics serve as a baseline for future researchers to report their improvements. Given the   \n41 diverse nature of research, it is important for the baseline to cover common approaches and common   \n42 metrics. This ensures that the achievements of future researchers can be effectively compared,   \n43 facilitating a standardized assessment of advancements in the field.\n\n# 44 1.2Contribution\n\n45 We address the existing gap and present our Chicks4FreeID dataset, which does not only support   \n46 the task of re-identification but also semantic and instance segmentation. We make this thoroughly   \n47 documented dataset freely accessible to the research community and the public. The dataset includes   \n48 54 individuals, of which 50 are chickens. Each occurrence is nearly pixel-perfectly segmented,   \n49 resulting in 1270 instance masks. Based on the cut-out crops of 1215 chicken instance masks,   \n50 we provide an initial baseline for the task of closed set re-identification. This allows the research   \n51 community to compare their methods and results effectively. In summary:\n\ni We provide a comprehensive overview of publicly available datasets for animal reidentification. ii We introduce the first publicly available dataset for chicken re-identification. ili We establish a baseline for closed set re-identification on the introduced dataset.\n\n# 2 Related work\n\n# 2.1 Animal re-identification\n\nAnimal re-identification, the task of identifying individual animals within one (or sometimes several) species, finds applications in various fields. Particularly in wildlife conservation efforts, where monitoring endangered species is crucial [6-9]. But also in livestock management, notably cattle [10-14] and yak [15]. Honeybees [4] and bumblebees [16, 17] have also been subject to investigation.\n\n62 Re-identification falls into one of two categories: closed set and open set. In closed set re  \n63 identification, all individuals are known from the beginning, and those to be identified can be   \n64 matched with identities of a predefined set. In open set re-identification, the identity of the individual   \n65 in question may not necessarily be part of a predefined set. It is possible to encounter completely   \n66 new, undocumented individuals. Such individuals must be annotated as a new identity and, upon   \n67 subsequent encounters, accurately matched.   \n68 While facial recognition is a prevalent method for re-identifying humans [18], the faces of animals   \n69 can likewise serve as a means to re-identify individuals, as has previously been demonstrated for   \n70 rhesus macaque [19], chimpanzee [20], cats [21], lions [22], dogs [23], giant pandas [8] and red   \n71 pandas [9]. However, animals frequently exhibit more distinctive visual traits beyond their faces. For   \n72 example, natural markings such as stripes [24-27] and scale pattrns [28] can serve as prominent   \n73 identifiers. But also specific body parts can contribute to distinguishing individuals, such as the fins   \n74 of dolphins [29] and sharks [30]. Similarly to how fingerprints differentiate humans, nose prints of   \n75 dogs have been utilized to uniquely identify individual dogs [31]. Conversely, little inter-individual   \n76 variability poses a challenge to the re-identification task. Species exhibiting minimal or subtle visual   \n77 distinctions between individuals are, for instance, (polar) bears [32, 33] or elephants [34]. Visual traits   \n78 play a pivotal role in animal re-identification within computer vision, serving as essential markers   \n79 for distinguishing individuals. However, the task is complex and extends beyond mere visual cues.   \n80 Factors such as lighting, perspective, body changes over time, and partially obscured body parts pose   \n81 additional challenges [5].   \n82 To further advance the field and aid the research community, [35] released the WildlifeDatasets   \n83 toolkit - an open-source toolkit for animal re-identification. It gathers publicly available animal   \n84 re-identification datasets in one place, in an effort to make them more easily accessible and to improve   \n85 usability. Included are various tools, i.e., for data handling and processing, algorithms relevant   \n86 to the task of re-identification, pretrained models, as well as evaluation methods. Therewith, they   \n87 address the prevailing absence of standardization across the literature and facilitate comparability   \n88 and reproducibility of results. Within their work, they also introduce a new state-of-the-art, the   \n89 MegaDescriptor, notably the first foundation model for animal re-identification. Likewise, [36] present   \n90 an open-source re-identification method initially developed for sea stars, which was successfully   \n91 extended to seven mammalian species without adjustments. They also report state-of-the-art results.   \n92 Moreover, [37] introduced Tri-AI, a system designed for the rapid detection, identification, and   \n93 tracking of individuals from a wide range of primate species. The system is capable of processing   \n94 both video footage and still images. The task of re-identification is closely related to tracking, where   \n95 individuals are detected and tracked across various video frames. During tracking, individuals often   \n96 need to be re-identified after leaving and re-entering the field of vision.\n\n# 97 2.2 Re-identification datasets\n\n98 A review of existing resources revealed fewer than 40 publicly available datasets for animal re  \n99 identification. This leads to the conclusion that a significant number of animal species are not yet   \n100 covered, including chickens. Birds in general seem to be underrepresented in this domain, with only a   \n101 couple of datasets available [38, 39]. In fact, a noticeable focus lies on marine life [40-50]. However,   \n102 cattle are the most frequently featured species [11, 51-55], with much of the data collected by the   \n103 same group of researchers.   \n104 Table 1 provides a summary of the publicly accessible datasets found, arranged by year. Each entry   \n105 details the name of the dataset (Dataset'), the associated publication (\\*Publ.\"), and species focus   \n106 (Species\"). \"IDs\" denotes the number of unique identities present within the dataset. Additionally,   \n107 the total number of annotated animal instances within all images of each dataset is noted (\"Annot.).   \n108 An indication $( ^ { * } )$ of whether the data was derived from video sources is given as well. For ease of   \n109 access, a direct link to each dataset is provided (\\*Avail. at'). Although all of the datasets are publicly   \n110 accessible, some are released under licenses that are relatively restrictive.\n\nTable 1: Publicly available animal re-identification datasets, arranged by date of publication. An asterisk $( ^ { * } )$ marks data derived from video footage.   \n\n<table><tr><td>Year</td><td>Publ.</td><td>Dataset</td><td>IDs</td><td>Species</td><td>Annot.</td><td>Avail. at</td></tr><tr><td></td><td> ours</td><td>Chicks4FreeID</td><td>50, 2, 2</td><td>chicken, duck, rooster</td><td>1215, 40, 15</td><td>[56]</td></tr><tr><td>2024</td><td>[28]</td><td>SeaTurtleID2022</td><td>438</td><td>sea turtle</td><td>8729</td><td>[40]</td></tr><tr><td>2023</td><td>[3]</td><td>Mammal Club (IISD)</td><td>218</td><td>11 terrestrial mammal species*</td><td>33612</td><td>[57]</td></tr><tr><td>2023</td><td>[58]</td><td>Multi-pose dog dataset</td><td>192</td><td>dog</td><td>1657</td><td>[59]</td></tr><tr><td>2023</td><td>[32]</td><td> PolarBearVidID</td><td>13</td><td> polar bear*</td><td>138363</td><td>[60]</td></tr><tr><td>2023</td><td>[36]</td><td>Sea Star Re-ID</td><td>39, 56</td><td>common starfish, Australian cushion star</td><td>1204, 983</td><td>[41]</td></tr><tr><td>2022</td><td>[61]</td><td>Animal-Identification-</td><td>58, 26, 9</td><td>pigeon*, pig*, Koi fish*</td><td>12671, 6184,</td><td>[39]</td></tr><tr><td>2022</td><td></td><td>from-Video Beluga ID</td><td>788</td><td> beluga whale</td><td>1635 5902</td><td></td></tr><tr><td>2022</td><td>n.a. n.a.</td><td>Happywhale</td><td>15587</td><td>30 different species of whales and dolphins</td><td>51033</td><td>[42]</td></tr><tr><td>2022</td><td>n.a.</td><td>Hyiena ID.</td><td>256</td><td>spotted hyena</td><td>3129</td><td>[43] [62]</td></tr><tr><td>2022</td><td>n.a.</td><td>Leopard ID</td><td>430</td><td>African leopard</td><td>6805</td><td>[63]</td></tr><tr><td>2022</td><td>[64]</td><td>SeallD</td><td>57.</td><td>Saimaa ringed seal</td><td>2080</td><td>[44]</td></tr><tr><td>2022</td><td>[65]</td><td>SeaTurtleIDHeads</td><td> 400</td><td>sea turtle</td><td>7774</td><td>[45]</td></tr><tr><td>2022</td><td>n.a.</td><td>Turtle Recall</td><td>100</td><td></td><td>2145</td><td></td></tr><tr><td>2021</td><td>[66]</td><td>Cow Dataset</td><td>13</td><td>sea turtle cow</td><td>3772</td><td>[46] [11]</td></tr><tr><td>2021</td><td>[13]</td><td>Cows2021</td><td>182</td><td>Holstein-Friesian cattle*</td><td>13784</td><td>[51]</td></tr><tr><td>2021</td><td>[67]</td><td>Giraffe Dataset</td><td>62</td><td>giraffe</td><td>624</td><td>[68]</td></tr><tr><td>2021</td><td>[8]</td><td>iPanda-50</td><td> 50</td><td> giant panda</td><td>6874</td><td>[69]</td></tr><tr><td>2020</td><td>[26]</td><td>AAU Zebrafish Dataset</td><td>6.</td><td>zebrafish*</td><td>6672</td><td>[70]</td></tr><tr><td>2020</td><td>[37]</td><td>Animal Face Dataset</td><td>1040</td><td>41 primate species</td><td>102399</td><td>[71]</td></tr><tr><td>2020</td><td>[24]</td><td>ATRW</td><td>92</td><td>Amur tiger*</td><td>3649</td><td></td></tr><tr><td>2020</td><td>[73]</td><td> Lion Face Dataset</td><td>94</td><td>lion</td><td>740</td><td>[72]</td></tr><tr><td>2020</td><td>[74]</td><td>NDD20</td><td>44, 82</td><td>bottlenose and white-beaked dolphin, white-beaked dolphin (underwater)*</td><td>2201, 2201</td><td>[22] [47]</td></tr><tr><td>2020</td><td>[73]</td><td> Nyala Data</td><td>237</td><td>nyala</td><td>1942</td><td>[75]</td></tr><tr><td>2020</td><td>[14]</td><td> OpenCows2020</td><td>46</td><td>Holstein-Friesian cattle*</td><td>4736</td><td>[52]</td></tr><tr><td>2019</td><td>[76]</td><td>Bird individuallD</td><td>30, 10,10</td><td>sociable weaver, great tit, zebra finch</td><td>51934</td><td>[38]</td></tr><tr><td>2019</td><td>[23]</td><td>Dog Face Dataset</td><td>1393</td><td></td><td>8363</td><td>[77]</td></tr><tr><td>2018</td><td>[21]</td><td>Cat Individual Images</td><td>518</td><td>dog cat.</td><td>13536</td><td>[78]</td></tr><tr><td>2018</td><td>[79]</td><td>Fruit Fly Dataset</td><td>60</td><td>fruit fly*</td><td>2592000</td><td>[80]</td></tr><tr><td>2018</td><td>n.a.</td><td>HumpbackWhaleID</td><td> 5004</td><td>humpback whale</td><td>15697</td><td>[48]</td></tr><tr><td>2018</td><td>[19]</td><td>MacaqueFaces</td><td>34</td><td>rhesus macaque*</td><td>6280</td><td>[81]</td></tr><tr><td>2017</td><td>[12]</td><td>AerialCattle2017</td><td>23</td><td>Holstein-Friesian cattle*</td><td>46340</td><td>[53]</td></tr><tr><td>2017</td><td>[12]</td><td>FriesianCattle2017</td><td>89.</td><td>Holstein-Friesian cattle*</td><td>940</td><td>[54]</td></tr><tr><td>2017</td><td>[25]</td><td>GZGC</td><td>2056</td><td> plains zebra and Masai giraffe</td><td>6925</td><td>[82]</td></tr><tr><td>2016</td><td>[20]</td><td>C-Tai</td><td>78</td><td>chimpanzee</td><td>5078</td><td>[83]</td></tr><tr><td>2016</td><td>[20]</td><td>C-Zoo</td><td>24</td><td>chimpanzee</td><td>2109</td><td>[83]</td></tr><tr><td>2016</td><td>[10]</td><td>FriesianCattle2015</td><td>40</td><td>Holstein-Friesian cattle*</td><td>377</td><td>[55]</td></tr><tr><td>2015</td><td>n.a.</td><td>Right Whale Recognition</td><td>447</td><td>North Atlantic right whale</td><td>4544</td><td>[49]</td></tr><tr><td>2011</td><td>[27]</td><td>StripeSpotter</td><td>45.</td><td>plains and Grevy&#x27;s zebra</td><td>820.</td><td>[27]</td></tr><tr><td>2009</td><td>[84]</td><td>Whale Shark ID</td><td>543</td><td>whale shark</td><td>7693</td><td>[50]</td></tr></table>\n\n# 111 3 The Chicks4FreeID dataset\n\n# 3.1 Data\n\nThe Chicks4FreeID dataset contains top-down view images of individually segmented and annotated chickens, with some images also featuring roosters and ducks. Each image is accompanied by a color-coded semantic segmentation mask that classifies pixel values by animal category (chicken, rooster, duck) and background, as well as binary segmentation mask(s) for the animal instance(s) depicted. Additionally, the dataset includes preprocessed cut-out crops (detailed in Section 3.5) of the respective animal instances. Figure 2 gives a first overview of the dataset.\n\n![](images/1f3cef0d1a313a13bd1ee1d10647759b8cb18e094aab22eedd8817ead1eb65da.jpg)  \nFigure 2: Dataset overview.\n\nVarious coops of private households were visited to photograph chickens. Among these coops, two additionally accommodate a rooster each, while another houses two ducks. A total of 677 images were captured using two similar models of cameras: the \"Sony CyberShot DSC-RX100 VI' and the \"Sony CyberShot DSC-RX100 I'. The resolution of the images stands at $3 6 4 8 \\mathrm { x } 5 4 7 2$ pixels. Every image includes at least one chicken, ensuring no images without chickens are part of the dataset. It was collected over the span of one year, however all images of a coop were shot within one day. In other words, all photos of a given individual were taken on the same day. The images were captured from a top-down view perspective, aiming to capture the plumage. The dataset is not limited to a single breed of chicken, ensuring a certain level of variability.\n\n# 3.3 Annotation\n\nWe utilized Labelbox [85] under a free educational license for manual data annotation.\n\n31 Instances and background For each animal instance appearing within an image, a segmentation   \n32 was meticulously hand-crafted by a human annotator. No AI has been used during the annotation   \n33 process to ensure high-quality, nearly pixel-perfect instance masks. The instance masks include   \n34 the comb, head, beak, and plumage. Feet were excluded as rings could give away the identity.   \n35 Feet and scattered feathers are considered part of the background, along with any visible objects or   \n36 living beings that are not chickens, roosters, or ducks. Compared to conventional bounding boxes,   \n37 instance masks offer the advantage of better supporting the subsequent re-identification process.   \n38 The background can be easily removed as it might contain unwanted clues about the identity of the   \n39 chickens. Furthermore, the provided masks render the dataset well-suited for instance segmentation   \n40 tasks as well.   \n141 Animal categories  Each instance of an animal was assigned to one of three animal categories.   \n142 These are \"chicken\", \"rooster\", and \"duck\". Roosters and especially ducks serve as exceptions within   \n143 the predominantly chicken-based collection. This characteristic potentially positions the dataset as a   \n144 resource for anomaly detection as well..   \n145 Identities and coops The identities of the subjects were meticulously studied prior to photography,   \n146 closely monitored throughout the image capture process, and ultimately assigned by a human annota  \n147 tor. The ground truth annotation was performed without the use of any algorithm. In cases where the   \n148 human annotator could not assign an identity, the instance was labeled as identity \"Unknown'. It is   \n149 essential to clarify that the label \\*Unknown\" does not imply the presence of a new individual. Instead,   \n150 it represents an unidentified individual from the closed set, more precisely, from the annotated coop.   \n151 Each image contains one or more chickens, all of which are individually identified by their unique   \n152 names. Roosters and ducks are each also uniquely named. Furthermore, each instance is explicitly   \n153 annotated to indicate the specific coop to which it belongs.   \n154 Visibility rating Acknowledging varying visibility of the subjects (chickens, roosters, ducks) within   \n155 the images, each appearance has been manually assigned a visibility rating, categorized as either   \n156 \"bad\", \"good', or \"best\". The best' rating includes segmentation instances that fully display the   \n157 subject from the desired top-down perspective, and those where only an insignificant part is missing,   \n158 such as the very tip of the tail feathers. Instances that include only small parts of the subject and on   \n159 which the subject is difficult to recognize fall under the \"bad' rating. All remaining segmentation   \n160 instances, that do not qualify as \"bad\"' or \"best\", are rated as \"good\".\n\n# 3.4Composition\n\n162 The dataset comprises a collection of 677 images, featuring a total of 50 distinct chicken, 2 rooster,   \n163 and 2 duck identities distributed across 11 different coops. A total of 1270 instances were obtained   \n164 by segmenting 1215 appearances (instances) of chickens, alongside 15 roosters and 40 ducks.   \n165 Each instance is of a certain animal category (chicken', \"rooster', \"duck\") and was assigned   \n166 the corresponding coop (1-11), visibility (best\", \"good', \"bad') and identity (1 of 54 names or   \n167 \"Unknown'). It is important to mention that no \\*Unknown' instances are present in the \"best\" or   \n168 \"good' subset. The ground truth identity for all instances in these subsets is, therefore, known. Figure   \n169 3 illustrates the number of instances for each individual, as well as the visibility rating of the instances.   \n170 It starts with the individual with the most instances in the \"best' subset and is arranged in descending   \n171 order. The most represented chicken in the \\*best\" subset is Mirmir with 27 instances, whereas Isolde   \n172 is the least represented chicken with 4 instances.\n\n![](images/4251895efffdb6c05d8f2bae5ac2edb6d89761266034d30a7757851e878a5d2f.jpg)  \nFigure 3: Visibility distributions for all instances of each individual. Ducks and roosters are marked with an asterisk $( ^ { * } )$\n\n# 1733.5Preprocessing\n\n174 The following steps describe the preprocessing procedure to obtain the cut-out crops for the re  \n175 identification task. For all individuals captured in an image, a bounding box is created based on the   \n176 instance masks. In the first step, both the image and the mask are cropped (to the area of interest   \n177 contained in the bounding box) to focus solely on the individual (see Figure 4: Step 1). The cropped   \n178 mask is then used to remove the background from the cropped image (Step 2). Finally, the resulting   \n179 image is adjusted to a square shape for ease of use and consistency (Step 3). The resulting resolutions   \n180 remain as is, with no resizing taking place.\n\n![](images/a33ac9f2ae0aea7596dc2d31f02bfa5ef001c1044a1e674cd233663f0e7a8710.jpg)  \nFigure 4: Data preprocessing pipeline for subsequent re-identification.\n\n# 181 4Experiments\n\n# 4.1 Dataset, split and augmentation\n\nFor the closed set re-identification experiments, we utilize preprocessed cut-out crops as described in Section 3.5. To focus solely on all 50 chicken identities, the four identities of ducks and roosters were excluded. By removing instances of visibility level \"good' and \"bad', we ensure that only instances with \"best' visibility are included. The utilized \"best\" subset does not contain any \"Unknown' instances. The number of chicken instances contained in the \"best\"' subset is 793.\n\nThe employed data is split into 630 train pairs and 163 test pairs of cut-out crops and the assigned identities. To ensure that the testing set does not introduce any new identities, we include all possible identities in the training set. For a fair evaluation on all identities, the train/test split is stratified, i.e., each identity has the same fixed percentage of its cut-out crops allocated to the test set. Consequently, identities with a higher total number of crops will contribute more to the test set compared to identities with fewer crops, ensuring proportional representation across all identities. The corresponding subset on Hugging Face is \"chicken-re-id-best-visibility\".\n\nTo avoid data leakage, it is important to apply data augmentation only after a train-test split is established. This ensures that augmented versions of the same original image do not appear in both sets. We dynamically apply the following data augmentation during training on the \"chickenre-id-best-visibility' subset: rotation, flip, RandAugment [86], and random color-jitter. No data augmentation is applied to the test set.\n\n# 4.2 Baseline approaches\n\nTo establish a baseline for the closed set re-identification task, we test three different approaches on our dataset. Each approach involves two steps. First, a feature extractor generates embeddings for the cut-out crops. Second, the resulting feature vectors (embeddings) are then passed to a classifier to ultimately assign the identities. We test each approach with a variation of two classifiers: $\\mathbf { k }$ -Nearest Neighbor (k-NN) and a linear classifier adapted from the Lightly library [87] (MIT License). All feature extractors were fed with images at an input resolution of $3 8 4 \\times 3 8 4$ pixels and each approach was run three times. The baseline results were obtained on 64GB shared memory Apple M3 Max Chips (2023) running PyTorch 2.3.0 with MPS acceleration.\n\nMegaDescriptorThe employed MegaDescriptor-L-384 [35] (CC BY-NC 4.0 license [88]) is a state-of-the-art feature extractor for animal re-identification from the WildlifeDatasets toolkit (MIT license). It is based on the Swin Transformer architecture [89] and was pretrained on diverse datasets featuring various animal species. However, it has not been trained on chicken data and we did not fine-tune it either. A notable hyperparameter choice made by the MegaDescriptor-L384 authors is the ArcFace [90] loss function, which aims to aid in building meaningful embeddings. We selected the frozen MegaDescriptor-L-384 model over DINOv2 [91] and CLIP [92] due to its better performance on unseen animal domains, as reported by the authors. Their evaluation included cattle as an example of an unseen domain [35].\n\n218 Swin TransformerWe utilize the swin_large_patch4_window12_384 architecture [89] as imple.   \n219 mented in [93]. We train it from scratch on the Chicks4FreeID dataset in a fully supervised manner.   \n220 The training process and hyperparameters mirror those used to build the MegaDescriptor-L384, which   \n221 also employs the same Swin Transformer architecture. Unlike the frozen MegaDescriptor-L384,   \n222 which was trained on a variety of animal datasets, we now train the Swin architecture exclusively on   \n223 our own dataset. The Swin Transformer itself is based on the Vision Transformer architecture.   \n224 Vision TransformerFinally, we employ the ViT-B/16 [94] architecture, as implemented in [95],   \n225 and train it on the Chicks4FreeID dataset in a fully supervised manner with a simple cross-entropy loss.   \n226 We adopted the effective hyperparameter settings as used in Lightly's benchmarks [87], including   \n227 optimizer and scheduler choices, for our experiments. The difference between the Swin Transformer\n\n8 and the Vision Transformer lies in how they handle image data; the Swin Transformer uses a hierarchical structure with shifted windows to capture local and global features, while the Vision Transformer treats images as sequences of patches, relying on self-attention mechanisms throughout.\n\n# 4.3 Evaluation\n\nFor all baselines, we provide three of the most common metrics for closed set animal re-identification. These are: mAP (mean Average Precision), Top-1 accuracy (ratio of correct predictions versus total predictions), and Top-5 accuracy (accuracy of the correct class being within the top 5 predictions) as implemented in TorchMetrics [96].\n\n# 4.4Baseline results and discussion\n\n37 The results for all baseline approaches and the respective variations are summarized in Table 2.   \n38Overall, the experiments yield good results but still leave room for improvement.   \n239 Both the Swin Transformer and Vision Transformer architectures, when trained from scratch, outper.   \n240 formed the frozen MegaDescriptor model. Additionally, linear classifiers consistently outperformed   \n241 $\\mathbf { k }$ NN classifiers. This indicates that performance scales with the level of supervision, which aligns   \n242 with expectations.\n\nTable 2: Baseline results for the closed set re-identification experiments. The highest scores for each metric are in blue.   \n\n<table><tr><td>Feature extractor</td><td>Training</td><td>Epochs</td><td>Classifier</td><td>mAP</td><td>Top-1</td><td>Top-5</td></tr><tr><td>MegaDescriptor [35]</td><td> pretrained, frozen</td><td></td><td>k-NN</td><td>0.649  0.044</td><td>0.709  0.026</td><td>0.924  0.027</td></tr><tr><td>MegaDescriptor [35]</td><td>pretrained, frozen</td><td>-</td><td> linear</td><td>0.935  0.005</td><td>0.883  0.009</td><td>0.985  0.003</td></tr><tr><td>Swin Transformer [89]</td><td>from scratch</td><td>200</td><td>k-NN</td><td>0.837  0.062</td><td>0.881  0.041</td><td>0.983  0.010</td></tr><tr><td>Swin Transformer [89]</td><td>from scratch</td><td>200</td><td> linear</td><td>0.963  0.022</td><td>0.922  0.042</td><td>0.987  0.012</td></tr><tr><td>Vision Transformer [94]</td><td>from scratch</td><td>200</td><td>k-NN</td><td>0.893  0.010</td><td>0.923  0.005</td><td>0.985  0.019</td></tr><tr><td>Vision Transformer [94]</td><td>from scratch</td><td>200</td><td>linear</td><td>0.976  0.007</td><td>0.928  0.002</td><td>0.990  0.012</td></tr></table>\n\nThe gap between the MegaDescriptor, a model from a different domain (trained on different species), and those trained from scratch on the target species suggests that the Chicks4FreeID dataset likely has unique characteristics not present in the datasets used to pretrain the MegaDescriptor. Thus, our dataset could enhance the underlying data distribution used to train general animal re-identification models like the MegaDescriptor.\n\nAdditionally, there is a small improvement in scores between the Vision Transformer over the Swin ) architecture, which was used to train the MegaDescriptor. The slightly better performance of the ) Vision Transformer might be due to two reasons: First, we observed a more stable training process for the Vision Transformer (cross-entropy loss) than for the Swin Transformer (ArcFace loss). Therefore ? we believe that training a more straightforward approach allows for easier convergence on a small 3 dataset like ours. Second, we replaced the standard classification head of the Vision Transformer with a simple linear layer. Since a simple linear layer has limited discriminative power, achieving j good overall performance suggests the presence of good embeddings, which was confirmed by the ; embedding evaluation using k-NN.\n\n# 5 Conclusion\n\n# 5.1 Findings\n\nThe Chicks4FreeID benchmark dataset was introduced. To the best of our knowledge, it is the very first publicly available dataset for chicken re-identification. The dataset is well-annotated and released under the relatively unrestrictive CC BY 4.0 license. It contains 1270 instance annotations of 54 individuals - 50 individuals and 1215 of the instances are chicken. The 677 images, which depict mainly chickens from 11 different coops and various breeds, were individually captured rather than\n\n264 derived from video. The dataset was created systematically, with manual annotation and instance-to  \n265 individual assignments based on expert knowledge, without the use of automated methods, ensuring   \n266 reliable ground truth annotations. Instead of providing merely bounding boxes that might include   \n267 parts of the background or other individuals, we offer preprocessed cut-out crops based on precise   \n268 segmentations of the instances. While the main use case of the dataset is the re-identification of   \n269 chickens, it also supports semantic and instance segmentation. In addition to instance and semantic   \n270 segmentation masks, information on identity, animal category, and coop, the dataset also includes   \n271 a visibility rating of the instances, accounting for occlusions. For the task of closed set chicken   \n272 re-identification, we established a baseline on the dataset, achieving Top-1 accuracy scores up to   \n273 0.928, Top-5 accuracy scores up to 0.990, and mAP scores up to 0.976 with the Vision Transformer.   \n274 The experiments suggest that the introduced dataset could be a valuable resource for training more   \n275 robust (general) animal re-identification systems.\n\n# 5.2Limitations\n\nOne clear limitation of the dataset is its size. With 1215 instance annotations of 50 chicken individuals, it is comparatively small. There also exists an imbalance within the classes (individuals), with the number of instances ranging from 4 to 27 in the \"best\" visibility subset. For chicken breeds with minimal inter-individual variability (e.g., uniform plumage), having more individuals and more instances of each individual would likely aid in re-identification. Additionally, all images of a given chicken were taken on the same day, so changes in appearance over time were not captured. An open question is the dataset's applicability to industrial farming, where thousands of chickens of a single breed are typically kept. A specialized dataset for such breeds could potentially be more suitable for commercial applications. Furthermore, the chicken breeds included in the Chicks4FreeID dataset are not exhaustive, despite their variability. The specific breeds were not annotated because they could not always be accurately determined..\n\n# 5.3Future work\n\nTo further enhance the Chicks4FreeID dataset and address its current limitations, future work could focus on several promising directions. Expanding the dataset to include a larger number of individuals and an even broader range of breeds would enhance its robustness and generalizability. Enriching the metadata with detailed breed-specific information could provide additional context. Methods to automatically create new labeled samples from existing data using generative AI, as proposed in [97], could be evaluated for their potential to aid in expanding the dataset. To capture changes in appearance over time due to factors such as molting, growth, and environmental conditions, individuals from the dataset may be photographed again, provided they are still alive. Similarly, new individuals added to the dataset could be photographed repeatedly over time. The versioning system of the dataset facilitates potential expansions and continuous improvements, ensuring its ongoing relevance and applicability for future research. However, the challenge of long-term data collection persists, as free-range chickens often fall prey to wild predators (e.g., foxes or raccoons). Another interesting direction for future work would be the investigation of models trained on the dataset and their applicability to industrial farming settings with crowded conditions and chickens of a single breed. On a final note, we envision the Chicks4FreeID dataset being utilized by established and aspiring researchers alike, i.e., in future research, contributing to the development of chicken-specific and multi-species re-identification systems, as well as being used for practicing purposes.\n\n# 306 Acknowledgments and Disclosure of Funding\n\n307 We are immensely thankful to the kind chicken owners who opened their coops for our research,   \n308 allowing us to collect data and generously offering us fresh eggs. Each of your chickens has made a   \n309 unique and valuable contribution to the advancement of science.   \n310 References   \n311 [1] A. M. Guhl and L. L. Ortman, \"Visual Patterns in the Recognition of Individuals among Chickens, The   \n312 Condor, vol. 55, no. 6, pp. 287-298, 11 1953. [Online]. Available: https:/doi.org/10.2307/1365008   \n313 [2] E. T. Psota, T. Schmidt, B. Mote, and L. C. Perez, \"Long-term tracking of group-housed livestock using   \n314 keypoint detection and map estimation for individual animal identification, Sensors, vol. 20, no. 13, p. 23,   \n315 2020. [Online]. Available: https://doi.org/10.3390/s20133670   \n316 [3] W. Lu, Y. Zhao, J. Wang, Z. Zheng, L. Feng, and J. Tang, \"Mammalclub: An annotated wild mammal   \n317 dataset for species recognition, individual identification, and behavior recognition,' Electronics, vol. 12,   \n318 no. 21, p. 14, 2023. [Online]. Available: https://www.mdpi.com/2079-9292/12/21/4506   \n319 [4] J. Chan, H. Carrion, R. Megret, J. L. A. Rivera, and T. Giray, Honeybee re-identification in video: New   \n320 datasets and impact of self-supervision, in Proceedings of the 17th International Joint Conference on   \n321 Computer Vision, Imaging and Computer Graphics Theory and Applications (VISIGRAPP 2022) - Volume   \n322 5: VISAPP, INSTICC. Avenida de S. Francisco Xavier, Lote 7 Cv. C, 2900-616 Setubal, Portugal:   \n323 SciTePress, 2022, pp. 517-525.   \n324 [5] M. Vidal, N. Wolf, B. Rosenberg, B. P. Harris, and A. Mathis, Perspectives on Individual Animal   \n325 Identification from Biology and Computer Vision, Integrative and Comparative Biology, vol. 61, no. 3,   \n326 pp. 900-916, 05 2021. [Online]. Available: https://doi.org/10.1093/icb/icab107   \n327 [6] P. Kulits, J. Wall, A. Bedetti, M. Henley, and S. Beery, \"Elephantbook: A semi-automated human-in  \n328 the-loop system for elephant re-identification,\" in Proceedings of the 4th ACM SIGCAS Conference on   \n329 Computing and Sustainable Societies, ser. COMPASS '21.New York, NY, USA: Association for   \n330 Computing Machinery, 2021, p. 88-98. [Online]. Available: https://doi.org/10.1145/3460112.3471947   \n331 [7] O. Moskvyak, F. Maire, F. Dayoub, A. O. Armstrong, and M. Baktashmotlagh, Robust re-identification   \n332 of manta rays from natural markings by learning pose invariant embeddings,' in 2021 Digital Image   \n333 Computing: Techniques and Applications (DICTA). Piscataway, NJ: IEEE, 2021, pp. 1-8.   \n334 [8] L. Wang, R. Ding, Y. Zhai, Q. Zhang, W. Tang, N. Zheng, and G. Hua, \"Giant panda identification, IEEE   \n335 Transactions on Image Processing, vol. 30, pp. 2837-2849, 2021.   \n336 [9] Q. He, Q. Zhao, N. Liu, P. Chen, Z. Zhang, and R. Hou, Distinguishing individual red pandas from their   \n337 faces, in Pattern Recognition and Computer Vision, Z. Lin, L. Wang, J. Yang, G. Shi, T. Tan, N. Zheng,   \n338 X. Chen, and Y. Zhang, Eds.Cham: Springer International Publishing, 2019, pp. 714-724.   \n339 [10] W. Andrew, S. Hannuna, N. Campbell, and T. Burghardt, \"Automatic individual holstein friesian cattle   \n340 identification via selective local coat patern matching in rgb-d imagery,\" in 2016 IEEE International   \n341 Conference on Image Processing (ICIP). Piscataway, NJ: IEEE, 2016, pp. 484-488.   \n342 [11] S. Li, L. Fu, Y. Sun, Y. Mu, L. Chen, J. Li, and H. Gong, \"Cow dataset, https://doi.org/10.6084/m9.   \n343 figshare.16879780, 2021.   \n344 [12] W. Andrew, C. Greatwood, and T. Burghardt, Visual localisation and individual identification of holstein   \n345 friesian cattle via deep learning, in 2017 IEEE International Conference on Computer Vision Workshops   \n346 (ICCVW). Piscataway, NJ: IEEE, 2017, pp. 2850-2859.   \n347 [13] J. Gao, T. Burghardt, W. Andrew, A. W. Dowsey, and N. W. Campbell, \\*Towards self-supervision for video   \n348 identification of individual holstein-friesian cattle: The cows2021 dataset, 2021.   \n349 [14] W. Andrew, J. Gao, S. Mullan, N. Campbell, A. W. Dowsey, and T. Burghardt, \\*Visual identification of   \n350 individual holstein-friesian cattle via deep metric learning,\" Computers and Electronics in Agriculture,   \n351 vol. 185, p. 106133, 2021. [Online]. Available: https://www.sciencedirect.com/science/article/pi/   \n352 S0168169921001514   \n353 [15] T. Zhang, Q. Zhao, C. Da, L. Zhou, L. Li, and S. Jiancuo, Yakreid-103: A benchmark for yak re  \n354 identification,\" in 2021 IEEE International Joint Conference on Biometrics (IJCB). Piscataway, NJ: IEEE,   \n355 2021, pp. 1-8.   \n356 [16] F. Tausch, S. Stock, J. Fricke, and O. Klein, \"Bumblebee re-identification dataset,\" in 2020 IEEE Winter   \n357 Applications of Computer Vision Workshops (WACVW). Piscataway, NJ: IEEE, 2020, pp. 35-37.   \n358 [17] P. Borlinghaus, F. Tausch, and L. Rettenberger, \"A purely visual re-id approach for bumblebees   \n359 (bombus terrestris),\" Smart Agricultural Technology, vol. 3, p. 100135, 2023. [Online]. Available:   \n360 https://www.sciencedirect.com/science/article/pii/S2772375522000995   \n361 [18] M. Ye, J. Shen, G. Lin, T. Xiang, L. Shao, and S. C. H. Hoi, Deep learning for person re-identification: A   \n362 survey and outlook, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 6, pp.   \n363 2872-2893, 2022.   \n364 [19] C. L. Witham, \"Automated face recognition of rhesus macaques,' Journal of Neuroscience   \n365 Methods, vol. 300, pp. 157-165, 2018, measuring Behaviour 2016. [Online]. Available:   \n366 https://www.sciencedirect.com/science/article/pii/S0165027017302637   \n367 [20] A. Freytag, E. Rodner, M. Simon, A. Loos, H. S. Kuhl, and J. Denzler, Chimpanzee faces in the wild: Log  \n368 euclidean cnns for predicting identities and atributes of primates, in Pattern Recognition, B. Rosenhahn   \n369 and B. Andres, Eds.Cham: Springer International Publishing, 2016, pp. 51-63.   \n370 [21] T-Y. Lin and Y.-F. Kuo, \"Cat face recognition using deep learning,\" St. Joseph, MI, 2018.   \n371 [22] N. Dlamini and T. L. v. Zyl, \"Lion face dataset,\" https://github.com/tvanzyl/wildlife_reidentification/, 2020,   \n372 mara Masia project, Kenya.   \n373 [23] G. Mougeot, D. Li, and S. Jia, \"A deep learning approach for dog face verification and recognition, in   \n374 PRICAI 2019: Trends in Artificial Intelligence, A. C. Nayak and A. Sharma, Eds. Cham: Springer   \n375 International Publishing, 2019, pp. 418-430.   \n376 [24] S. Li, J. Li, H. Tang, R. Qian, and W. Lin, \\*Atrw: A benchmark for amur tiger re-identification in   \n377 the wild, in Proceedings of the 28th ACM International Conference on Multimedia, ser. MM '20.   \n378 New York, NY, USA: Association for Computing Machinery, 2020, p. 2590-2598. [Online]. Available:   \n379 https://doi.org/10.1145/3394171.3413569   \n380 [25] J. Parham, J. Crall, C. Stewart, T. Berger-Wolf, and D. Rubenstein, Animal population censusing at scale   \n381 with citizen science and photographic identification, AAAI Spring Symposium - Technical Report, vol.   \n382 SS-17-01 - SS-17-08, pp. 37 - 44, 2017. [Online]. Available: http://arks.princeton.edu/ark:/88435/pr1s791   \n383 [26] J. B. Haurum, A. Karpova, M. Pedersen, S. H. Bengtson, and T. B. Moeslund, \"Re-identification of   \n384 zebrafish using metric learning,\" in 2020 IEEE Winter Applications of Computer Vision Workshops   \n385 (WACVW). Piscataway, NJ: IEEE, 2020, pp. 1-11.   \n386 [27] M. Lahiri, C. Tantipathananandh, R. Warungu, D. I. Rubenstein, and T. Y. Berger-Wolf, Biometric animal   \n387 databases from field photographs: identification of individual zebra in the wild, in Proceedings of the 1st   \n388 ACM International Conference on Multimedia Retrieval, ser. ICMR '11. New York, NY, USA: Association   \n389 for Computing Machinery, 2011. [Online]. Available: https://doi.org/10.1145/1991996.1992002   \n390 [28] L. Adam, V. Cermak, K. Papafitsoros, and L. Picek, \"Seaturtleid2022: A long-span dataset for reliable sea   \n391 turtle re-identification, in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer   \n392 Vision (WACV). Piscataway, NJ: IEEE, January 2024, pp. 7146-7156.   \n393 [29] S. Bouma, M. D. Pawley, K. Hupman, and A. Gilman, \\*Individual common dolphin identification via   \n394 metric embedding learning, in 2018 International Conference on Image and Vision Computing New   \n395 Zealand (IVCNZ). Piscataway, NJ: IEEE, 2018, pp. 1-6.   \n396 [30] B. Hughes and T. Burghardt, \"Automated visual fin identification of individual great white sharks,\"'   \n397 International Journal of Computer Vision, vol. 122, no. 3, pp. 542-557, May 2017. [Online]. Available:   \n398 https://doi.org/10.1007/s11263-016-0961-y   \n399 [31] H. B. Bae, D. Pak, and S. Lee, Dog nose-print identification using deep neural networks, IEEE Access,   \n400 vol. 9, pp. 49 141-49 153, 2021.   \n401 [32] M. Zuerl, R. Dirauf, F. Koeferl, N. Steinlein, J. Sueskind, D. Zanca, I. Brehm, L. v. Fersen, and B. Eskofier,   \n402 \"Polarbearvidid: A video-based re-identification benchmark dataset for polar bears,' Animals, vol. 13,   \n403 no. 5, p. 16, 2023. [Online]. Available: https://www.mdpi.com/2076-2615/13/5/801   \n404 [33] M. Clapham, E. Miller, M. Nguyen, and C. T. Darimont, \"Automated facial recognition for wildlife that   \n405 lack unique markings: A deep learning approach for brown bears, Ecology and Evolution, vol. 10, no. 23,   \n406 pp. 12 883-12 892, 2020. [Online]. Available: https://onlinelibrary.wiley.com/doi/abs/10.1002/ece3.6840   \n407 [34] M. Korschens and J. Denzler, \\*Elpephants: A fine-grained dataset for elephant re-identification, in 2019   \n408 IEEE/CVF International Conference on Computer Vision Workshop (ICCvw).Piscataway, NJ: IEEE,   \n409 2019, pp. 263-270.   \n410 [35] V. Cermak, L. Picek, L. Adam, and K. Papafitsoros, \\*WildlifeDatasets: An Open-Source Toolkit for Animal   \n411 Re-Identification, in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision   \n412 (WACV). Piscataway, NJ: IEEE, January 2024, pp. 5953-5963.\n\n[36] O. Wahltinez and S. J. Wahltinez, \"An open-source general purpose machine learning framework for individual animal re-identification using few-shot learning,\" Methods in Ecology and Evolution, vol. 15, no. 2, pp. 373-387, 2024.   \n[37] S. Guo, P. Xu, Q. Miao, G. Shao, C. A. Chapman, X. Chen, G. He, D. Fang, H. Zhang, Y. Sun, Z. Shi, and B. Li, \"Automatic identification of individual primates with deep learning techniques,' iScience, vol. 23, no. 8, p. 32, Aug 2020. [Online]. Available: https://doi.org/10.1016/j.isci.2020.101412   \n[38] A. C. Ferreira, L. R. Silva, F. Renna, H. B. Brandl, J. P. Renoult, D. R. Farine, R. Covas, and C. Doutrelant, \"Bird individualid,' https://github.com/AndreCFerreira/Bird_individualID, 2020.   \n[39] L. I. Kuncheva, F. Williams, S. L. Hennessey, and J. J. Rodriguez, \"Animal-identification-from-video,\" https://github.com/LucyKuncheva/Animal-Identification-from-Video, 2022..   \n[40] L. Adam, V. Cermak, K. Papafitsoros, and L. Picek, \"Seaturtleid, https://www.kaggle.com/datasets! wildlifedatasets/seaturtleid2022, 2022.   \n[41] O. Wahltinez, \"Sea star re-id,' https://lila.science/sea-star-re-id-2023/, 2023.   \n[42] w. Me, \"Beluga id,' https:/lila.science/datasets/beluga-id-2022/, 2022, info@wildme.org.   \n[43] T. Cheeseman, K. Southerland, W. Reade, and A. Howard, \"Happywhale - whale and dolphin identification,' https://kaggle.com/competitions/happy-whale-and-dolphin, 2022.   \n[44] E. Nepovinnykh, \"Sealid,\" https://doi.org/10.23729/0f4a3296-3b10-40c8-9ad3-Ocf00a5a4a53, 2022, lappeenranta University of Technology, School of Engineering Science yhteiset.   \n[45] K. Papafitsoros, L. Adam, V. Cermak, and L. Picek, \"Seaturtleid,\" https://www.kaggle.com/datasets! wildlifedatasets/seaturtleidheads, 2022.   \n[46] W. T. Watch and L. O. Conservation, Turtle recall: Conservation challenge, https://zindi.africa/ competitions/turtle-recall-conservation-challenge/data, 2022.   \n[47] C. Trotter, G. Atkinson, M. Sharpe, K. Richardson, A. S. McGough, N. Wright, B. Burville, and P. Berggren, The northumberland dolphin dataset 2020, https://doi.org/10.25405/data.ncl.c.4982342, 2020, newcastle Uniyversity. Collection.   \n[48] A. Howard, inversion, K. Southerland, and T. Cheeseman, Humpback whale identification, 2018. [Online]. Available: https:/kaggle.com/competitions/humpback- whale-identification   \n[49] C. B. Khan, Shashank, and W. Kan, Right whale recognition, 2015. [Online]. Available: https://kaggle.com/competitions/noaa-right-whale-recognition   \n[50] J. Holmberg, B. Norman, and Z. Arzoumanian, \"Whale shark id, https:/lila.science/datasets! whale-shark-id, 2020, info@ wildme.org.   \n[51] J. Gao, T. Burghardt, W. Andrew, A. W. Dowsey, and N. W. Campbell, \"Cows2021, https:/doi.org/10. 5523/bris.4vnrca7qw1642qlwxjadp87h7, 2021.   \n[52] W. Andrew, J. Gao, S. Mullan, N. Campbell, A. W. Dowsey, and T. Burghardt, \"Opencows2020, https: |/doi.org/10.5523/bris.10m32xl88x2b61zlkkgz3fml17, 2020.   \n[53] W. Andrew, C. Greatwo0d, and T. Burghardt, \"Aerialcattle2017, 2017. [Online]. Available: https://doi.org/10.5523/bris.3owflku95bxsx24643cybxu3qh   \n[54] \"Friesiancattle2017, 2017. [Online]. Available: https://doi.org/10.5523/bris. 2yizcfbkuv4352pzc32n54371r   \n[55] W. Andrew, S. Hannuna, N. Campbell, and T. Burghardt, Friesiancattle2015, 2016. [Online]. Available: https://doi.org/10.5523/bris.wurzq71kfm561ljahbwjhx9n3   \n[56] D. Kern, T. Schiele, U. Klauck, A. DeSilva, and W. Ingabire, \"Chicks4freeid, https:/huggingface.co/ datasets/dariakern/Chicks4FreeID, 2024.   \n[57] W. Lu, Y. Zhao, J. Wang, Z. Zheng, L. Feng, and J. Tang, Mammalclub,' https:/ithub.com/WJ-0425/ MammalClub, 2023, download: https://pan.baidu.com/s/1in8xJxdjoNMNb3yuKbVDPA. (code: o5tq).   \n[58] Z. He, J. Qian, D. Yan, C. Wang, and Y. Xin, \\*Animal re-identification algorithm for posture diversity,' in ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Piscataway, NJ: IEEE, 2023, pp. 1-5. 63 [60] M. Zuerl, R. Dirauf, F. Koeferl, N. Steinlein, J. Sueskind, D. Zanca, I. Brehm, L. von Fersen, and 64 B. Eskofier, \\*PolarBearVidID: A Video-based Re-Identification Benchmark Dataset for Polar Bears,' Jan. 65 2023. [Online]. Available: https://doi.org/10.5281/zenodo.7564529   \n166 [61] L. I. Kuncheva, F. Williams, S. L. Hennessey, and J. J. Rodriguez, \"A benchmark database for animal re67 identification and tracking, in 2022 IEEE 5th International Conference on Image Processing Applications 68 and Systems (IPAS), vol. Five. Piscataway, NJ: IEEE, 2022, pp. 1-6.   \n169 [62] B. P. C. Trust, Hyiena id, https:/lila.science/datasets/hyena-id-2022l, 2022, panthera pardus CSV custom .70 export. Retrieved from African Carnivore Wildbook 2022-04-28. info@ wildme.org.   \n71 [63] \"Leopard id,\" https:/lila.science/datasets/leopard-id-20221, 2022, panthera pardus CSV custom 172 export. Retrieved from African Carnivore Wildbook 2022-04-28. info@ wildme.org.   \n173 [64] E. Nepovinnykh, T. Eerola, V. Biard, P. Mutka, M. Niemi, M. Kunnasranta, and H. Kalviainen, Sealid: 174 Saimaa ringed seal re-identification dataset,\" Sensors, vol. 22, no. 19, p. 11p, 2022. [Online]. Available: 175 https://www.mdpi.com/1424-8220/22/19/7602   \n76 [65] K. Papafitsoros, L. Adam, V. Cermak, and L. Picek, Seaturtleid: A novel long-span dataset highlighting 177 the importance of timestamps in wildlife re-identification,' 2022.   \n178 [66] S. Li, L. Fu, Y. Sun, Y. Mu, L. Chen, J. Li, and H. Gong, Individual dairy cow identification based on 179 lightweight convolutional neural network, PLoS ONE, vol. 16, no. 11, p. 13, 2021.   \n180 [67] V. Miele, G. Dussert, B. Spataro, S. Chamaille-Jammes, D. Allaine, and C. Bonenfant, 81 \"Revisiting animal photo-identification using deep metric learning and network analysis, Methods 182 in Ecology and Evolution, vol. 12, no. 5, pp. 863-873, 2021. [Online]. Available:  https: 183 //besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13577   \n184 [68] \"Giraffe dataset,\" https://plmlab.math.cnrs.fr/vmiele/animal-reid/, 2020, ftp link ftp:/bil.univ-lyon1. 85 fr/pub/datasets/miele2021.   \n186 [69] L. Wang, R. Ding, Y. Zhai, Q. Zhang, W. Tang, N. Zheng, and G. Hua, \"ipanda-50, https://github.com/ 187 iPandaDateset/iPanda-50, 2021.   \n188 [70] J. B. Haurum, A. Karpova, M. Pedersen, S. H. Bengtson, and T. B. Moeslund, \"Aau zebrafish re189 identification dataset, https://www.kaggle.com/datasets/aalborguniversity/aau-zebrafish-reid, 2020. 190 [71] S. Guo, P. Xu, Q. Miao, G. Shao, C. A. Chapman, X. Chen, G. He, D. Fang, H. Zhang, Y. Sun, Z. Shi, and 191 B. Li, \\*\\*Afd, https://doi.org/10.17632/z3x59pv4bz.2, 2020, mendeley Data Version 2.   \n192 [72] S. Li, J. Li, H. Tang, R. Qian, and W. Lin, \"Atrw (amur tiger re-identification in the wild),\" https: 193 //lila.science/datasets/atrw, 2020.   \n94 [73] N. Dlamini and T. L. v. Zyl, \"Automated identification of individuals in wildlife population using siamese 95 neural networks,\" in 2020 7th International Conference on Soft Computing and Machine Intelligence 196 (ISCMI).Piscataway, NJ: IEEE, 2020, pp. 224-228.   \n197 [74] C. Trotter, G. Atkinson, M. Sharpe, K. Richardson, A. S. McGough, N. Wright, B. Burville, and 198 P. Berggren, \\*NDD20: A large-scale few-shot dolphin dataset for coarse and fine-grained categorisation,\" 199 CoRR, vol. abs/2005.13359, p. 5, 2020. [Online]. Available: https://arxiv.org/abs/2005.13359   \ni00 [75] N. Dlamini and T. L. v. Zyl, \\*Nyala dataset,\" https://ithub.com/tvanzyl/wildife_reidentification/, 2020, i01 south African nature reserves.   \ni02 [76] A. C. Ferreira, L. R. Silva, F. Renna, H. B. Brandl, J. P. Renoult, D. R. Farine, R. Covas, i03 and C. Doutrelant, \"Deep learning-based methods for individual recognition in small birds,' i04 Methods in Ecology and Evolution, vol. 11, no. 9, pp. 1072-1085, 2020. [Online]. Available: i05 https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13436   \ni06 [77] G. Mougeot, D. Li, and S. Jia, Dog face dataset,\" https://github.com/GuillaumeMougeot/DogFaceNet, i07 2019.   \ni08 [78] T-Y. Lin and Y-F. Kuo, \"Cat individual images, https:/www.kaggle.com/datasets/timost1234/ i09 cat-individuals, 2018. i10 [79] J. Schneider, N. Murali, G. Taylor, and J. Levine, \"Can drosophila melanogaster tell who's who?\" PLoS 511 ONE, vol. 13, no. 10, p. 10, 2018.   \nj12 [80] .Dataset for: Can Drosophila melanogaster tell who's who? 2018. [Online]. Available: i13 https://doi.org/10.5683/SP2/JP4WDF   \n514 [81] C. L. Witham, Macaquefaces,' https://github.com/clwitham/MacaqueFaces, 2018.   \n515 [82] J. Parham, J. Crall, C. Stewart, T. Berger-Wolf, and D. Rubenstein, \"Great zebra and giraffe count id,\" 616 https://lila.science/datasets/great-zebra- giraffe-id, 2017, info@ wildme.org.   \n517 [83] A. Freytag, E. Rodner, M. Simon, A. Loos, H. S. Kuhl, and J. Denzler, \"Chimpanzee faces in the wild,\" 18 https://github.com/cvjena/chimpanzee_faces, 2016, acknowledgements: Tobias Deschner, Laura Aporius, i19 Karin Bahrke, Zoo Leipzig.   \ni20 [84] J. Holmberg, B. Norman, and Z. Arzoumanian, \"Estimating population size, structure, and residency i21 time for whale sharks rhincodon typus through collaborative photo-identification, Endangered Species i22 Research, vol. 7, no. 1, pp. 39-53, 2009.   \ni23 [85] Labelbox, \\*\"labelbox\", [Online] Available: https://labelbox.com, 2024.   \ni24 [86] E. D. Cubuk, B. Zoph, J. Shlens, and Q. Le, Randaugment: Practical automated data augmentation i25 with a reduced search space,\" in Advances in Neural Information Processing Systems, H. Larochelle, i26 M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, i27 pp. 18613-18 624. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/2020/file/ i28 d85b63ef0ccb114d0a3bb7b7d808028f-Paper.pdf   \ni29 [87] I. Susmelj, M. Heller, P. Wirth, J. Prescott, and M. E. et al., \"Lightly,\" 2020.   \n530 [88] V. Cermak, L. Picek, L. Adam, and K. Papafitsoros, Megadescriptor-1-384, https://huggingface.co/BVRA/ i31 MegaDescriptor-L-384, 2024.   \ni32 [89] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, \"Swin transformer: Hierarchical i33 vision transformer using shifted windows, in 2021 IEEE/CVF International Conference on Computer i34 Vision (ICCV). Los Alamitos, CA, USA: IEEE Computer Society, oct 2021, pp. 9992-10 002. [Online]. i35 Available: https://doi.ieeecomputersociety.org/10.1109/ICCV48922.2021.00986   \n536 [90] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, \"Arcface: Additive angular margin loss for deep face recognition, i37 in. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 538 2019.   \n539 [91] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, i40 A. El-Nouby, M. Assran, N. Ballas, W. Galuba, R. Howes, P.-Y. Huang, S.-W. Li, I. Misra, M. Rabbat, j41 V. Sharma, G. Synnaeve, H. Xu, H. Jegou, J. Mairal, P. Labatut, A. Joulin, and P. Bojanowski, Dinov2: j42 Learning robust visual features without supervision,' 2024, unpublished.   \nj43 [92] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, i44 J. Clark, G. Krueger, and I. Sutskever, Learning transferable visual models from natural language i45 supervision,' 2021, unpublished.   \ni46 [93] R. Wightman, Pytorch image models, https://github.com/rwightman/pytorch-image-models, 2019. i47 [94] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, j48 M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, \"An image is worth 16x16 words: j49 Transformers for image recognition at scale,\" in International Conference on Learning Representations, i50 2021. [Online]. Available: https://openreview.net/forum?id $\\ c =$ YicbFdNTTy   \ni51 [95] T. maintainers and contributors, Torchvision: Pytorch's computer vision library, https://github.com/ i52 pytorch/vision, 2016.   \ni53 [96] N. S. Detlefsen, J. Borovec, J. Schock, A. H. Jha, T. Koker, L. D. Liello, D. Stancl, C. Quan, M. Grechkin, i54 and W. Falcon, Torchmetrics - measuring reproducibility in pytorch,' Journal of Open Source Software, i55 vol. 7, no. 70, p. 4101, 2022. [Online]. Available: https://doi.org/10.21105/joss.04101   \ni56 [97] Y. Zhang, D. Zhou, B. Hooi, K. Wang, and J. Feng, \"Expanding small-scale datasets with i57 guided imagination,' in Advances in Neural Information Processing Systems, A. Oh, T. Naumann, i58 A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds., vol. 36. Curran Associates, Inc., i59 2023, pp. 76558-76618. [Online]. Available: https:/proceedings.neurips.cc/paper_files/paper/2023/file/ i60 f188a55392d3a7509b0b27f8d24364bb-Paper-Conference.pdf\n\n1. For all authors...\n\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]   \n(b) Did you describe the limitations of your work? [Yes] See Section 5.2.   \n(c) Did you discuss any potential negative societal impacts of your work? [N/A] It is a chicken dataset.   \n(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\n\n2. If you are including theoretical results...\n\n(a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A]\n\n3. If you ran experiments (e.g. for benchmarks)....\n\n(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See supplementary material.   \n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 4.   \n(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] See Table 2.   \n(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Section 4.2.\n\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n\n(a) If your work uses existing assets, did you cite the creators? [Yes] We used existing architectures, models and code. The creators were cited..   \n(b) Did you mention the license of the assets? [Yes] We mentioned the licenses in the paper and in the supplementary material.   \n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes] The Chicks4FreeID dataset. https://doi.org/10.57967/hf/2345   \n(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A]   \n(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]\n\n5. If you used crowdsourcing or conducted research with human subjects...\n\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]   \n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]   \n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]", "status": "success", "markdown_file": "full.md"}
