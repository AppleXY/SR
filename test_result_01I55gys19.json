{
  "paper_id": "01I55gys19",
  "pdf_url": "https://openreview.net/pdf?id=01I55gys19",
  "markdown_content": "# Few-Class Arena: A Benchmark for Efficient Selection of Vision Models and Dataset Difficulty Measurement\n\nAnonymous Author(s)   \nAffiliation   \nAddress   \nemail\n\n# Abstract\n\n1 A wide variety of benchmark datasets with many classes (80-1000) have been   \n2 created to assist Computer Vision architectural evolution. An increasing number of   \n3 vision models are evaluated with these many-class datasets. However, real-world   \n4 applications often involve substantially fewer classes of interest (2-10). This gap   \n5 between many and few classes makes it difficult to predict performance of the   \n6 few-class applications using models trained on the available many-class datasets.   \n7 To date, little has been offered to evaluate models in this Few-Class Regime. We   \n8 propose Few-Class Arena $( F C A )$ , as a unified benchmark with focus on testing.   \n9 efficient image classification models for few classes. We conduct a systematic   \n10 evaluation of the ResNet family trained on ImageNet subsets from 2 to 1000 classes,   \n11 and test a wide spectrum of Convolutional Neural Networks and Transformer   \n12 architectures over ten datasets by using our newly proposed FCA tool. Furthermore,   \n13 to aid an up-front assessment of dataset difficulty and a more efficient selection   \n14 of models, we incorporate a difficulty measure as a function of class similarity.   \n15 $F C A$ offers a new tool for efficient machine learning in the Few-Class Regime,   \n16 with goals ranging from a new efficient class similarity proposal, to lightweight   \n17 model architecture design, to a new scaling law. FCA is user-friendly and can be   \n18 easily extended to new models and datasets, facilitating future research work. Our   \n19 benchmark is available at https: //github. com/fewclassarena/fca.\n\n# 20 1 Introduction\n\n21The de-facto benchmarks for evaluating efficient vision models are large scale with many classes   \n22 (e.g. 1000 in ImageNet [1], 80 in COCO [2], etc.). Such benchmarks have expedited the advance of   \n23 vision neural networks toward efficiency [3, 4, 5, 6, 7, 8, 9, 10] with the hope of reducing the financial   \n24 and environmental cost of vision models [11, 12]. More efficient computation is facilitated by using   \n25 quantization [13, 14, 15], pruning [16, 17, 18, 19], and data saliency [20]. Despite efficiency   \n26 improvements such as these, many-class datasets are still the standard of model evaluation.   \n27 Real-world applications, however, typically comprise only a few number of classes (e.g, less than   \n28 10) [21, 22, 23] which we termed Few-Class Regime. To deploy a vision model pre-trained on large   \n29 datasets in a specific environment, it requires the re-evaluation of published models or even retraining   \n30 to find an optimal model in an expensive architectural search space [24].   \n31 One major finding is that, apart from scaling down model and architectural design for efficiency,   \n32 dataset difficulty also plays a vital role in model selection [25] (described in Section 4.3).\n\n![](images/6cd710906541476f5b147435f04346dca6cf6832d5806b39a8c1247e553ddfda.jpg)  \n(a) Accuracies for sub-models (blue) and full models (red).\n\n![](images/d676e5cc0b835cd647941b1a29935f49534b6edb2da7e3c1885a58611caad3f6.jpg)  \n(b) Zoomed window shows accuracy values and range for full and sub-models in the few-class range.\n\n(c) Zoomed window shows (c.1) drop of accuracy as $N _ { C L }$ decreases, (c.2) accuracy scales with model size for full models in the few-class range.\n\n![](images/51e1f83ff26bf21462e13198321df9c130a1f50a030a87400b91b9ae5cf3cfd9.jpg)  \n(d) Zoomed window shows (d.1) rising accuracy as $N _ { C L }$ decreases, (d.2) accuracy does not scale with model size for sub-models in the few-class range..\n\n![](images/d08a32a1b365cf0a3231e1fa4c0abd1f8f587930d052b89cda88b6aa82cc12b9.jpg)  \nFigure 1: Top-1 accuracies of various scales of ResNet, whose model sizes are shown in the legend, and whose plots vary from dark to light by decreasing size. Plots range along number of classes $N _ { C l }$ from the full ImageNet size (1000) down to the Few-Class Regime. Each model is tested on 5 subsets whose $N _ { C l }$ classes are randomly sampled from the original 1000 classes. (a) Plots for sub-models trained on subsets of classes (blue) and full models trained on all 1000 classes (red). (b) Zoomed window shows the standard deviation of subset's accuracies is much smaller than for the full model. (c.1) Full model accuracies drop when $N _ { C L }$ decreases. (c.2) Full model accuracies increase as model scales up in the Few-Class Regime. (d.1) Sub-model accuracies grow as $N _ { C L }$ decreases. (d.2) Sub-model accuracies do not increase when model scales up in the Few-Class Regime.\n\n33 Figure 1 summarizes several key findings under the Few-Class Regime. On the left graph in red   \n34 are accuracy results for a range of number of classes $N _ { C L }$ for what we call the \"full model\", that   \n35 is ResNet models pre-trained on the full 1000 classes of ImageNet (generally available from many   \n36 websites). On the right are accuracy results for what we call \"sub-models\", each of which is trained   \n37 and tested on the same $N _ { C L }$ , where this number of classes is sampled from the full dataset down to   \n38 the Few-Class Regime. Findings include the following. (a) Sub-models attain higher upper-bound   \n39 accuracy than full models. (b) The range of accuracy widens for full models at few-classes, which   \n40 increases the uncertainty of a practitioner selecting a model for few classes. In contrast, sub-models   \n41 narrow the range. (c) Full models follow the scaling law [26] in the dimension of model size - larger   \n42 models (darker red) have higher accuracy from many to few classes. (4) Surprisingly, the scaling law   \n43 is violated for sub-models in the Few-Class Regime (see the zoomed-in subplot) where larger models   \n44 (darker blue) do not necessarily perform better than smaller ones (lighter blue). From these plots,   \n45 our key insight is that, instead of using full models, researchers and practitioners in the Few-Class   \n46 Regime should use sub-models for selection of more efficient models.   \n47 However, obtaining sub-models involves computationally expensive training and testing cycles since   \n48 they need to be converged on each of the few-class subsets. By carefully studying and comparing the   \n49 experiment and evaluation setup of these works in the literature, we observe that, how models scale   \n50 down to Few-Class Regime is rarely studied. The lack of comprehensive benchmarks for few-class   \n51 research impedes both researchers and practitioners from quickly finding models that are the most   \n52 efficient for their dataset size. To fill this need, we propose a new benchmark, Few-Class Arena $( F C A )$   \n53 with the goal of benchmarking vision models under few-class scenarios. To our best knowledge, $F C A$   \n54is the first benchmark for such a purpose..   \n55 We formally define Few-Class Regime as a scenario where the dataset has a limited number of classes.   \n56 Real-world applications often comprise only a few number of classes (e.g. $N _ { C L } < 1 0$ or $1 0 \\%$ classes   \n57 of a dataset). Consequently, Few-Class Arena refers to a benchmark to conduct research experiments   \n58 to compare models in the Few-Class Regime. This paper focuses on the image classification task,   \n59 although Few-Class Regime can generalize to object detection and other visual tasks.\n\n60 Statement of Contributions. Four contributions are listed below:\n\nTo be best of our knowledge, we are the first to explore the problems in the Few-Class Regime and develop a benchmark tool Few-Class Arena (FCA) to facilitate scientific research, analysis, and discovery for this range of classes.   \n: We introduce a scalable few-class data loading approach to automatically load images and labels in the Few-Class Regime from the full dataset, avoiding the need to duplicate data points for every additional few-class subset.   \nWe incorporate dataset similarity as an inverse difficulty measurement in Few-Class Arena and propose a novel Silhouette-based similarity score named SimSs. By leveraging the visual feature extraction power of CLIP and DINOv2, we show that SimSS is highly correlated with ResNet performance in the Few-Class Regime with Pearson coefficient scores $\\ge 0 . 8 8$   \n: We conduct extensive experiments that comprise ten models on ten datasets and 2-1000 numbers of classes on ImageNet, totalling 1591 training and testing runs. In-depth analyses on this large body of testing reveal new insights in the Few-Class Regime.\n\n# 74 2 Related Work\n\n75 Visual Datasets and Benchmarks. To advance deep neural network research, a wealth of large-scale   \n76 many-class datasets has been developed for benchmarking visual neural networks over a variety of   \n77 tasks. Typical examples ' include 1000 classes in ImageNet [1] for image classification, and 80 object   \n78 categories in COCO [2] for object detection. Previous benchmarks also extend vision to multimodal   \n79 research such as image-text [27, 28, 29, 30]. While prior works often scale up the number of object   \n80 categories for general purpose comparison, studies [31, 32] raise a concern on whether models trained   \n81 on datasets with such a large number of classes (e.g. ImageNet) can be reliably transferred to real   \n82 world applications often with far fewer classes. A close work to ours is vision backbone comparison   \n83 [33] whose focus is on model architectures. Our perspective differs in a focus on cases with fewer   \n84 number of classes, which often better aligns with real-world scenarios.\n\nDataset Difficulty Measurement. Research has shown the existence of inherent dataset difficulty [32] for classification and other analytic tasks. Efficient measurement methods are proposed to characterize dataset difficulty using Silhouette Score [34], K-means Frechet inception distance [35, 36, 37], and Probe nets [25]. Prior studies have proposed image quality metrics using statistical heuristics, including peak signal-to-noise ratio (PSNR) [38], structural similarity (SsIM) Index [39], and visual information fidelity VIF [40]. A neuroscience-based image diffculty metric [32] is defined as the minimum viewing time related to object solution time (OST) [41]. Another type of difficulty measure method consists of additional procedures such as c-score [42], prediction depth [43], and adversarial robustness [44]. Our work aligns with the line of research [45, 46, 47] involving similarity-based difficulty measurements: similar images are harder to distinguish from each other while dissimilar images are easier. Previous studies are mainly in the image retrieval context [48, 49, 50]. Similarity score is used in [51] with the limitation that a model serving similarity measurement has to be trained for one dataset. We push beyond this limit by leveraging large vision models that learn general visual features using CLIP [52] and DINOv2 [53]. The study [32] shows that CLIP generalizes well to both easy and hard images, making it a good candidate for measuring\n\n100 image difficulty. Supported by the evidence that better classifiers can act as better perceptual feature   \n101 extractors [54], in later sections we show how CLIP and DINOv2 will be used as our similarity base   \n102 function.   \n103 Despite the innovation of difficulty measure algorithms on many-class datasets, little attention has   \n104 been paid to leveraging these methods in the Few-Class Regime. We show that, as the number of   \n105 classes decreases, sub-dataset difficulty in the Few-Class Regime plays a more critical role in efficient   \n106 model selection. To summarize, unlike previous work on many-class benchmarks and difficulty   \n107 measurements, our work takes few-class and similarity-based dataset difficulty into consideration,   \n108 and in doing so we believe the work pioneers the development of visual benchmark dedicated to   \n109 research in the Few-Class Regime.\n\n# o 3 Few-Class Arena (FCA)\n\nWe introduce the Few-Class Arena (FCA) benchmark in this section. In practice, we have integrated FCA into the MMPreTrain framework [55], implemented in Python3 and Pytorch?.\n\n# 3.1 Goals\n\n1. Generality. All vision models and existing datasets for classification should be compatible in this framework. In addition, users can extend to custom models and datasets for their needs.\n\n2. Efficiency. The benchmark should be time- and space-efficient for users. The experimental setup for the few-class benchmark should be easily specified by a few hyper-parameters (e.g. number of classes). Since the few-class regime usually includes sub-datasets extracted from the full dataset, the benchmark should be able to locate those sub-datasets without generating redundant duplicates for reasons of storage efficiency. For time-efficiency, it should conduct training and testing automatically through use of user-specified configuration files, without users' manual execution.\n\n3. Large-Scale Benchmark. The tool should allow for large-scale benchmarking, including training and testing of different vision models on various datasets when the number of classes varies.\n\n# 3.2Few-Class Dataset Preparation\n\nFew-Class Arena provides an easy way to prepare datasets in the Few-Class Regime. By leveraging the MMPreTrain framework, users only need to specify the parameters of few-class subsets in the configuration files, which includes the list of models, datasets, number of classes $( N _ { C L } )$ , and the number of seeds $( N _ { S } )$ . Few-Class Arena generates the specific model and dataset configuration files for each subset, where subset classes are randomly extracted from the full set of classes, as specified by the seed number. Note that only one copy of the full, original dataset is maintained during the whole benchmarking life cycle because few-class subsets are created through the lightweight configurations, thus maximizing storage efficiency. We refer readers to the Appendix and the publicly released link for detailed implementations and use instructions.\n\n# 3.3 Many-Class Full Dataset Trained Benchmark\n\n5 We conducted large-scale experiments spanning ten popular vision models (including CNN and   \n6 ViT architectures) and ten common datasets 3. Except for ImageNet1K, where pre-trained model   \n7 weights are available, we train models in other datasets from scratch. While different models'   \n138 training procedures may incur various levels of complexity (particularly in our case for MobileNet   \n139 V3 and Swin Transformer V2 base), we have endeavored to minimize changes in the existing training   \n140 pipelines from MMPreTrain. The rationale is that if a model exhibits challenges in adapting it to a   \n141 dataset, then it is often not a helpful choice for a practitioner to select for deployment.   \n142 Results are summarized in Table 1. We make several key observations: (1) models in different datasets   \n143 (in rows) yield highly variable levels of performance by Top-1 accuracy; (2) no single best model   \n144 (bold, in columns) exists across all datasets; and (3) model rankings vary across various datasets.   \n145 The first two observations are consistent with the findings in [25, 31]. For (1), it suggests there exists   \n146 underlying dataset-specific difficulty. To capture this characteristic, we adopt the reference dataset   \n147 classification difficulty number (DCN) [25] to refer to the empirically highest accuracy achieved in   \n148 a dataset from a finite number of models shown in Table 1 and Figure 2 (a). For observation (3),   \n149 we can examine the rankings among the ten models of ResNet50 and EfficientNet V2 in Figure 2   \n150 (b). ResNet50's ranking varies dramatically for the different datasets, for instance ranking 7th on   \n151 ImageNet1K and 1st on Quickdraw345. This ranking variability is also observed in other models   \n152 (see all models in the Appendix). However, a common practice is to benchmark models - even for   \n153 efficiency - on large datasets, especially ImageNet1K. The varied dataset rankings in our experiments   \n154 expose the limitations of such a practice, further supporting our new benchmark paradigm, especially   \n155 in the Few-Class Regime. In later sections, we leverage DCN and image similarity for further analysis.\n\n<table><tr><td>Dataset</td><td>RN50 [56]</td><td>VGG16 CNv2 [57]</td><td>[58]</td><td>INCv3 [59]</td><td>EFv2 [4]</td><td>SNv2 [9]</td><td>MNv3 [7]</td><td>ViTb [60]</td><td>[61]</td><td>SWv2b MViTs| [10]</td><td>DCN [25]</td></tr><tr><td>GT43 [62]</td><td>99.85</td><td>96.60</td><td>99.83</td><td>99.78</td><td>99.86</td><td>99.87</td><td>5.98</td><td>99.31</td><td>99.78</td><td>99.69</td><td>99.87</td></tr><tr><td>CF100 [63]</td><td>74.56</td><td>71.12</td><td>85.89</td><td>75.97</td><td>77.05</td><td>77.89</td><td>1.00</td><td>32.65</td><td>78.49</td><td>76.51</td><td>85.89</td></tr><tr><td>IN1K [1]</td><td>76.55</td><td>71.62</td><td>84.87</td><td>77.57</td><td>85.01</td><td>69.55</td><td>67.66</td><td>82.37</td><td>84.6</td><td>78.25</td><td>85.01</td></tr><tr><td>FD101 [64]</td><td>83.76</td><td>75.82</td><td>63.80</td><td>83.96</td><td>80.82</td><td>79.36</td><td>0.99</td><td>52.21</td><td>84.30</td><td>82.23</td><td>84.30</td></tr><tr><td>CT101 [65]</td><td>77.70</td><td>74.99</td><td>77.52</td><td>77.52</td><td>77.82</td><td>84.13</td><td>76.58</td><td>59.59</td><td>78.82</td><td>80.06</td><td>84.13</td></tr><tr><td>CT256 [66]</td><td>65.07</td><td>59.08</td><td>73.57</td><td>66.09</td><td>62.80</td><td>68.13</td><td>22.63</td><td>44.23</td><td>67.28</td><td>65.80</td><td>73.57</td></tr><tr><td>QD345 [67]</td><td>69.14</td><td>19.86</td><td>62.86</td><td>68.25</td><td>68.81</td><td>67.32</td><td>0.72</td><td>19.67</td><td>66.54</td><td>68.76</td><td>69.14</td></tr><tr><td>CB200 [68]</td><td>45.86</td><td>21.26</td><td>27.61</td><td>45.58</td><td>44.48</td><td>53.95</td><td>47.22</td><td>23.73</td><td>54.52</td><td>58.46</td><td>58.46</td></tr><tr><td>ID67 [69]</td><td>53.75</td><td>26.01</td><td>33.21</td><td>45.95</td><td>43.85</td><td>54.72</td><td>49.10</td><td>30.51</td><td>48.58</td><td>54.05</td><td>54.72</td></tr><tr><td>TT47 [70]</td><td>30.43</td><td>12.55</td><td>6.49</td><td>14.20</td><td>21.17</td><td>43.83</td><td>2.18</td><td>31.38</td><td>33.94</td><td>24.41</td><td>43.83</td></tr></table>\n\nTable 1: Top-1 accuracy across ten models in ten datasets. Models are trained and tested on full datasets with their original number of classes (e.g. 1K from ImageNet1K); this is denoted in the last few digits of the abbreviation of the dataset name. The best score is highlighted in bold while the second best is underlined for each dataset.\n\n(a) Top-1 accuracy and DCN in ten full datasets.\n\n![](images/f81c639d38c8182e9219a76f9a57a377bbc45526162b8211abdff4885ec83078.jpg)  \nFigure 2: Many-Class Full Dataset Benchmark.\n\n![](images/6dc868ac8d2871624baac46cdf0edaca745d4e9aac751c5142aef3fd6c8d76d2.jpg)  \n(b) Ranking of ResNet50 (RN50) and EfficientNet V2 (EFv2) across 10 datasets by Top-1 acc.\n\n156 In the next subsections, we introduce three new types of benchmarks: (1) Few-Class, Full Dataset   \n157 Trained Benchmark (FC-Full), which benchmarks vision models trained on the full dataset with the   \n158 original number of classes; (2) Few-Class, Subset Trained Benchmark (FC-Sub), which benchmarks   \n159 vision models trained on subsets of a fewer number of classes than the full dataset, and (3) Few-Class   \n160 Similarity Benchmark (FC-Sim), which benchmarks image similarity methods and their correlation   \n161 with model performance.\n\nTraditionally, a large number of models are trained and compared on many-class datasets. However, results for such benchmarks are not directly useful to the Few-Class Regime and many real-world scenarios. Therefore, we introduce the Few-Class Full Dataset Trained Benchmark (FC-Full), with the objective of effortlessly conducting large-scale experiments and analyses in the Few-Class Regime.\n\n167 The procedure of FC-Full consists of two main stages. In the first stage, users select the models   \n168 and datasets upon which they would like to conduct experiments. They can choose to download   \n169 pre-trained model weights, which are usually available on popular model hubs (PyTorch Hub [71],   \n170 TensorFlow Hub [72], Hugging Face [73], MMPreTrain [55] etc.). In case of no pre-trained weights   \n171 available from public websites, users can resort to the option of training from scratch. To that end,   \n172 our tool is designed and implemented to generate bash scripts for easily configurable and modifiable   \n173 training through the use of configuration files.   \n174 In the second stage, users conduct benchmarking in the Few-Class Regime. By specifying the list of   \n175 classes, Few-Class Arena automatically loads pre-trained weights of the chosen models and evaluates   \n176 performance of the models on the selected datasets. Note that this process is accomplished through   \n177 configuration files created by the user's specifications, thus enabling hundreds of experiments to be   \n178 launched by a single command. This dramatically reduces human effort that would otherwise be   \n179 expended to run these experiments without Few-Class Arena.\n\n# 3.5Few-Class Subset Trained Benchmark (FC-Sub)\n\n31 Our study in Figure 1 (red lines) reveals the limits of existing pre-trained models in the Few-Class   \n32 Regime. To facilitate further research and analyze the upper bound performance in the Few-Class   \n33 Regime, we introduce the Few-Class Subset Trained Benchmark (FC-Sub).\n\nFC-Sub follows a similar procedure to FC-Full, except that, when evaluating a model in a subset with a specific number of classes, that model should have been trained on that same subset. Specifically, in Stage One (described for FC-Full, users specify models, datasets and the list of number of classes in configuration files. Then Few-Class Arena generates bash scripts for model training on each subset. In Stage two, Few-Class Arena tests each model in the same subset that it was trained on.\n\n# 3.6Few-Class Similarity Benchmark (FC-Sim)\n\nOne objective of our tool is to provide the Similarity Benchmark as a platform for researchers to design custom similarity scores for efficient comparison of models and datasets.\n\nThe intrinsic image difficulty of a dataset affects a model's classification performance (and human) [74, 75, 32]. We show - as is intuitive - that the more similar two images are, the more difficult it is for a vision classifier to make a correct prediction. This suggests that the level of similarity of images in a dataset can be used as a proxy for a dataset difficulty measure. In this section, we first adopt and provide the basic formulation of similarity, the baseline of a similarity metric. Then we propose a Similarity-Based Silhouette Score to capture the characteristic of image similarity in a dataset.\n\n198 We first adopt the basic similarity formulation from [51]. Intra-Class Similarity ${ S _ { \\alpha } ^ { ( C ) } }$ is defined as a   \n199 scalar describing the similarity of images within a class by taking the average of all the distinct class   \n200 pairs in $C$ , while Inter-Class Similarity denotes a scalar describing the similarity among images in   \n201 two different classes $C _ { 1 }$ and $C _ { 2 }$ . For a dataset $D$ , these are defined as the mean of their similarity   \n202 scores over all classes, respectively:\n\n$$\nS _ { \\alpha } ^ { ( D ) } = \\frac { 1 } { | L | } \\sum _ { l \\in L } S _ { \\alpha } ^ { ( C _ { l } ) } = \\frac { 1 } { | L | \\times | P ^ { ( C _ { l } ) } | } \\sum _ { l \\in L } \\sum _ { i , j \\in C _ { l } ; } \\cos ( \\mathbf { Z } _ { i } , \\mathbf { Z } _ { j } ) ,\n$$\n\n$$\nS _ { \\beta } ^ { ( D ) } = \\frac { 1 } { | P ^ { ( D ) } | } \\sum _ { a , b \\in L ; a \\neq b } S _ { \\beta } ^ { ( C _ { a } , C _ { b } ) } = \\frac { 1 } { | P ^ { ( D ) } | \\times | P ^ { ( C _ { 1 } , C _ { 2 } ) } | } \\sum _ { a , b \\in L ; a \\neq b } \\sum _ { i \\in C _ { 1 } , j \\in C _ { 2 } } \\cos ( \\mathbf { Z } _ { i } , \\mathbf { Z } _ { j } ) .\n$$\n\n:04 where $| L |$ is the number of classes in a dataset, $Z _ { i }$ is the visual feature of an image. $i$ $| P ^ { ( C ) } |$ is the :05 total number of distinct image pairs in class $C$ $| P ^ { ( D ) } |$ is the total number of distinct class pairs, and :06 $\\left| P ^ { ( C _ { 1 } , C _ { 2 } ) } \\right|$ is the total number of distinct image pairs excluding same-class pairs.\n\nAveraging these similarities provides a single scalar score at the class or dataset level. However, 3 this simplicity neglects other cluster-related information that can better reveal the underlying dataset ) difficulty property of a dataset. In particular, the (1) tightness of a class cluster and (2) distance to ) other classes of class clusters, are features that characterize the inherent class difficulty, but are not captured by $S _ { \\alpha }$ or $S _ { \\beta }$ alone.\n\n212 To compensate the aforementioned drawback, we adopt the Silhouette Score (Ss) [34, 76]: $S S ( i ) =$   \n213 max(-g. where ()is the ilhouette Scor of the data ponti, (i) is the average dissimilarit   \n214 between $i$ and other instances in the same class, and $b ( i )$ is the average dissimilarity between $i$ and   \n215 other data points in the closest different class.   \n216 Observe that the above Intra-Class Similarity $S _ { \\alpha } ^ { ( C ) }$ already represents the tightness of the class $( C )$   \n217 therefore $a ( i )$ can be replaced with the inverse of Intra-Class Similarity $a ( i ) = - S _ { \\alpha } ( i )$ . For the   \n21e second term $b ( i )$ we adopt the preiously deined Intr Clss Similarity $S _ { \\beta } ^ { ( C _ { 1 } , C _ { 2 } ) }$ and introduce a new   \n219 similarity score as Nearest Inter-Class Similarity $S _ { \\beta } ^ { \\prime } { } ^ { ( C ) }$ , which is a scalar describing the similarity   \n220 among instances between class $C$ and the closest class of each instance in. $C$ . The dataset-level   \n221 Nearest Iner-Cass imilrit sis expesed is\n\n$$\n\\boldsymbol { S } _ { \\beta } ^ { \\prime ( D ) } = \\frac { 1 } { | L | } \\sum _ { l \\in L } \\boldsymbol { S } _ { \\beta } ^ { \\prime ( C _ { l } , \\hat { C } _ { l } ) } = \\frac { 1 } { | L | \\times | P ^ { ( C _ { l } , \\hat { C } _ { l } ) } | } \\sum _ { l \\in L } \\sum _ { i \\in C _ { l } , j \\in \\hat { C } _ { l } } \\cos ( \\mathbf { Z } _ { i } , \\mathbf { Z } _ { j } ) .\n$$\n\n222 where $\\hat { C }$ is the set of the nearest class to. $C$ $( { \\hat { C } } \\neq C )$ . To summarize, we introduce our novel   \n223 Similarity-Based Silhouette Score $S i m S S ^ { 4 }$ ..\n\n$$\nS i m S S ^ { ( D ) } = \\frac { 1 } { | L | \\times | C _ { l } | } \\sum _ { i \\in C _ { l } } \\frac { S _ { \\alpha } ( i ) - S ^ { \\prime } { } _ { \\beta } ( i ) } { m a x ( S _ { \\alpha } ( i ) , S ^ { \\prime } { } _ { \\beta } ( i ) ) } .\n$$\n\n# 24 4Experimental Results\n\n# 4.1Results on FC-Full\n\nIn this section, we present the results of FC-Full. A model trained on the dataset with its original number of classes (e.g. 1000 in ImageNet1K) is referred to as a full-class model. These experiments are designed to understand how full-class model performance changes when the number of classes $N _ { C l }$ decreases from many to few classes. We analyze the results of DCN-Full, shown in Figure 3 (details of all models are presented in the Appendix), and we make two key observations when $N _ { C l }$ reduces to the Few-Class Regime (from right to left). (1) The best performing models do not always increase its accuracy for fewer classes, as shown by the solid red lines that represent the average of DCN for each $N _ { C l }$ . (2) The variance, depicted by the light red areas, of the best models broaden dramatically for low $N _ { C l }$ , especially for $N _ { C l } < 1 0$\n\nBoth observations support evidence of the limitations of using the common many-class benchmark for application model selection in the Few-Class Regime, since it is not consistent between datasets that a model can be made smaller with higher accuracy. Furthermore, the large variance in accuracy means that prediction of performance for few classes is unreliable for this approach.\n\n# 4.2Results on FC-Sub\n\n)In this section, we show how using Few-Class Arena can help reveal more insights in the Few-Class I Regime to mitigate the issues of Section 4.1.\n\n![](images/487f1d72e555455b45315f4c2ff8bbe9d3d9ecbf6487ce82b534e84fb3bea925.jpg)  \nFigure 3: DCN-Full by Top-1 Accuracy $( \\% )$ $N _ { C l }$ ranges from many to 2.\n\n242 FC-Sub results are displayed in Figure 4. Recall that a sub-class model is a model trained on a subset   \n243 of the dataset where $N _ { C l }$ is smaller than the original number of classes in the full dataset. Observe   \n244 that in the Few-Class Regime (when $N _ { C l }$ decreases from 4 to 2) that: (1) DCN increases as shown by   \n245 the solid blue lines, and (2) variance reduces as displayed by the light blue areas.   \n246 The preceding observation for FC-Full 4.1 seems to contradict the common belief that, the fewer the   \n247 classes, the higher is the accuracy that a model can achieve. Conversely, the FC-Sub results do align   \n248 with this belief. We argue that a full-class model needs to accommodate many parameters to learn   \n249 features that will enable high performance across all classes in a many-class, full dataset. With the   \n250 same parameters, however, a sub-class model can adapt to finer and more discriminative features that   \n251 improve its performance when the number of target classes are much smaller.\n\n![](images/8e6ae81509c6f205711bda4a696291771839e4df93180119969676e9785129b3.jpg)  \nFigure 4: DCN-Sub (red) and DCN-Full (blue) by Top-1 Accuracy $( \\% )$ $N _ { C L }$ ranges from 2 to 4.\n\n# 252 4.3 Results on FC-Sim\n\n253 In this section, we analyze the use of $\\mathrm { S i m S S }$ (Equation 4) as proxy for few-class dataset difficulty.   \n254 Experiments are conducted on ImageNet1K using the ResNet family for the lower $N _ { C L } \\leq 1 0 \\%$ range   \n255 of the original 1000 classes, $N _ { C L } \\in \\{ 2 , 3 , 4 , 5 , 1 0 , 1 0 0 \\}$ , and the results are shown in Figure 5. Each   \n256 datapoint of DCN-Full (diamond in red) or DCN-Sub (square in blue) represents an experiment in a\n\n57 subset of a specific $N _ { C L }$ , where classes are sampled from the full dataset. For reproducible results, i8 we use seed numbers from 0 to 4 to generate 5 subsets for one $N _ { C L }$ by default. A similarity base 59 function $( s i m ( ) )$ is defined as the atomic function that takes a pair of images as input and outputs a i0 scalar that represents their image similarity..\n\nIn our experiments, we leverage the general visual feature extraction ability of CLIP (image $^ +$ text) [52] and DINOv2 (image) [53] by self-supervised learning. Specifically, a pair of images are fed into its latent space from which the the cosine score is calculated and normalized to 0 to 1. Note that we only use the Image Encoder in CLIP.\n\nComparing Accuracy and Similarity To evaluate $\\mathrm { S i m S S }$ , we compute the Pearson correlation coefficient (PCC) $( r )$ between model accuracy and SimSS. Results in Figure 5 (a) (b) show that $\\mathrm { S i m } { \\bf S } { \\bf S }$ is poorly correlated with DCN-Full ( $r = 0 . 1 8$ and $r = 0 . 2 6$ for CLIP and DINOv2) due to the large variance shown in Section 4.1. In contrast, SimSS is highly correlated with DCN-Sub (shown in blue squares), with $r = 0 . 9 0$ and $r = 0 . 8 8$ using CLIP (dashed) and DINOv2 (solid), respectively. The high PCC [77, 78] demonstrates that $\\mathrm { S i m S S }$ is a reliable metric to estimate few-class dataset difficulty, and this can help predict the empirical upper-bound accuracy of a model in the Few-Class Regime. Comparison between SimSS and all models can be found in the Appendix. Such a high correlation suggests this offers a reliable scaling relationship to estimate model accuracy by similarity for other values of $N _ { C L }$ without an exhaustive search. Due to the dataset specificity of the dataset difficulty property, this score is computed once and used for all times the same dataset is used. We have made available difficulty scores for many datasets at the Few-Class Arena site.\n\n![](images/0c203a06bb7a577ed81fc86acd4c391598e77b808f88fc34a78a80be550f44c4.jpg)  \nFigure 5: Pearson correlation coefficient $( r )$ between DCN and $\\mathrm { { s i m s s } }$ when $N _ { C l } \\in \\{ 2 , 3 , 4 , 5 , 1 0 , 1 0 0 \\}$ . DCNSub (blue squares) is more highly correlated than DCN-Full(red diamonds) with SimSS using both similarity base functions of CLIP (dashed line) and DINOv2 (solid line) with $r \\geq 0 . 8 8$\n\n# 277 5 Conclusion\n\nWe have proposed Few-Class Arena and a dataset difficulty measurement, which together form a benchmark tool to compare and select efficient models in the Few-Class Regime. Extensive experiments and analyses over 1500 experiments with 10 models on 10 datasets have helped identify new behavior that is specific to the Few-Class Regime as compared to for many-classes. One finding reveals a new $n _ { C l }$ -scaling law whereby dataset difficulty must be taken into consideration for accuracy prediction. Such a benchmark will be valuable to the community by providing both researchers and practitioners with a unified framework for future research and real applications.\n\n85 Limitations and Future Work. We note that the convergence of sub-models is contingent on various   \n86 factors in a training scheduler, such as learning rate. A careful tuning of training procedure may   \n87 increase a model's performance, but it shouldn't change the classification difficulty number drastically   \n88 since this represents a dataset's intrinsic difficulty property. The current difficulty benchmark supports   \n89 image similarity while in the future it can be expanded to other difficulty measurements [25]. As   \n90 CLIP and DINOv2 are trained toward general visual features, it is unclear if they will be appropriate   \n91 for other types of images such as sketches without textures in Quickdraw [67] . For this reason, a   \n92 universal similarity foundation model would be appealing that applies to any image type. In summary,   \n93 Few-Class Arena identifies a promising new path to achieve efficiencies that are focused on the   \n94 important and practical Few-Class Regime, establishing this as a baseline for future work.\n\nReferences   \n[1] Deng, J., W. Dong, R. Socher, et al. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009.   \n[2] Lin, T.-Y., M. Maire, S. Belongie, et al. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755. Springer, 2014.   \n[3] Tan, M., Q. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pages 6105-6114. PMLR, 2019.   \n[4] Tan, M., Q. L. Efficientnetv2: Smaller models and faster training. In International conference on machine learning, pages 10096-10106. PMLR, 2021.   \n[5] Sinha, D., M. El-Sharkawy. Thin mobilenet: An enhanced mobilenet architecture. In 2019 IEEE 1Oth annual ubiquitous computing, electronics & mobile communication conference (UEMCON), pages 0280-0285. IEEE, 2019.   \n[6] Sandler, M., A. Howard, M. Zhu, et al. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4510-4520. 2018.   \n[7] Howard, A., M. Sandler, G. Chu, et al. Searching for mobilenetv3. In Proceedings of the 1EEE/CVF international conference on computer vision, pages 1314-1324. 2019.   \n[8] Iandola, F. N., S. Han, M. W. Moskewicz, et al. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and $< 0 . 5 \\mathrm { m b }$ model size. arXiv preprint arXiv:1602.07360, 2016.   \n[9] Ma, N., X. Zhang, H.-T. Zheng, et al. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In Proceedings of the European conference on computer vision (ECCV), pages 116-131. 2018.   \n[10] Mehta, S., M. Rastegari. Mobilevit: Light-weight, general-purpose, and mobile-friendly vision transformer. arxiv 2021. arXiv preprint arXiv:2110.02178.   \n[11] Patterson, D., J. Gonzalez, Q. Le, et al. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350, 2021.   \n[12] Rae, J. W., S. Borgeaud, T. Cai, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.   \n[13] Gysel, P., J. Pimentel, M. Motamedi, et al. Ristretto: A framework for empirical study of resource-efficient inference in convolutional neural networks. IEEE transactions on neural networks and learning systems, 29(11):5784-5789, 2018.   \n[14] Han, S., H. Mao, W. J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.   \n[15] Leng, C., Z. Dou, H. Li, et al. Extremely low bit neural network: Squeeze the last bit out with admm. In Proceedings of the AAAI conference on artificial intelligence, vol. 32. 2018.   \n[16] Cheng, Y., D. Wang, P. Zhou, et al. A survey of model compression and acceleration for deep neural networks. arXiv preprint arXiv:1710.09282, 2017.   \n[17] Blalock, D., J. J. Gonzalez Ortiz, J. Frankle, et al. What is the state of neural network pruning? Proceedings of machine learning and systems, 2:129-146, 2020.   \n[18] Li, H., A. Kadav, I. Durdanovic, et al. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016.   \n[19] Shen, M., H. Yin, P. Molchanov, et al. Structural pruning via latency-saliency knapsack. arXiv preprint arXiv:2210.06659, 2022.   \n[20] Yeung, S., O. Russakovsky, G. Mori, et al. End-to-end learning of action detection from frame glimpses in videos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2678-2687. 2016.   \n[21] Shao, Z., L. Wang, Z. Wang, et al. Saliency-aware convolution neural network for ship detection in surveillance video. IEEE Transactions on Circuits and Systems for Video Technology, 30(3):781-794, 2020.   \n[22] A. Delplanque, P. L. J. L. J. T., S. Foucher. Multispecies detection and identification of african mammals in aerial imagery using convolutional neural networks. Remote Sensing in Ecology and Conservation, 8(April):166-179, 2022.   \n[23] Cai, Y., T. Luan, H. Gao, et al. Yolov4-5d: An effective and efficient object detector for autonomous driving. IEEE Transactions on Instrumentation and Measurement, 70:1-13, 2021.   \n[24] Scheidegger, F., L. Benini, C. Bekas, et al. Constrained deep neural network architecture search for iot devices accounting for hardware calibration. Advances in Neural Information Processing Systems, 32, 2019.   \n[25] Scheidegger, F., R. Istrate, G. Mariani, et al. Efficient image dataset classification difficulty estimation for predicting deep-learning accuracy. The Visual Computer, 37(6): 1593-1610, 2021.   \n[26] Kaplan, J., S. McCandlish, T. Henighan, et al. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.   \n[27] Lee, T., M. Yasunaga, C. Meng, et al. Holistic evaluation of text-to-image models. Advances in Neural Information Processing Systems, 36, 2024.   \n[28] Le, T., V. Lal, P. Howard. Coco-counterfactuals: Automatically constructed counterfactual examples for image-text pairs. Advances in Neural Information Processing Systems, 36, 2024.   \n[29] Laurengon, H., L. Saulnier, L. Tronchon, et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. Advances in Neural Information Processing Systems, 36, 2024.   \n[30] Bitton, Y., N. Bitton Guetta, R. Yosef, et al. Winogavil: Gamified association benchmark to challenge vision-and-language models. Advances in Neural Information Processing Systems, 35:26549-26564, 2022.   \n[31] Fang, A., S. Kornblith, L. Schmidt. Does progress on imagenet transfer to real-world datasets? Advances in Neural Information Processing Systems, 36, 2024.   \n[32] Mayo, D., J. Cummings, X. Lin, et al. How hard are computer vision datasets? calibrating dataset difficulty to viewing time. Advances in Neural Information Processing Systems, 36:11008 11036, 2023.   \n[33] Goldblum, M., H. Souri, R. Ni, et al. Battle of the backbones: A large-scale comparison of pretrained models across computer vision tasks. Advances in Neural Information Processing Systems, 36, 2024.   \n[34] Rousseeuw, P. J. Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. Journal of computational and applied mathematics, 20:53-65, 1987.   \n[35] Dowson, D., B. Landau. The frechet distance between multivariate normal distributions. Journal of multivariate analysis, 12(3):450-455, 1982.   \n[36] Heusel, M., H. Ramsauer, T. Unterthiner, et al. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.   \n[37] Lucic, M., K. Kurach, M. Michalski, et al. Are gans created equal? a large-scale study. Advances in neural information processing systems, 31, 2018.   \n[38] Hore, A., D. Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th international conference on pattern recognition, pages 2366-2369. IEEE, 2010.   \n[39] Wang, Z., A. C. Bovik, H. R. Sheikh, et al. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600-612, 2004.   \n[40] Sheikh, H. R., A. C. Bovik. Image information and visual quality. IEEE Transactions on image processing, 15(2):430-444, 2006.   \n[41] Kar, K., J. Kubilius, K. Schmidt, et al. Evidence that recurrent circuits are critical to the ventral stream's execution of core object recognition behavior. Nature neuroscience, 22(6):974-983, 2019.   \n[42] Jiang, Z., C. Zhang, K. Talwar, et al. Characterizing structural regularities of labeled data in overparameterized models. arXiv preprint arXiv:2002.03206, 2020.   \n[43] Baldock, R., H. Maennel, B. Neyshabur. Deep learning through the lens of example difficulty. Advances in Neural Information Processing Systems, 34:10876-10889, 2021.   \n[44] Goodfellow, I. J., J. Shlens, C. Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.   \n[45] Arun, S. Turning visual search time on its head. Vision Research, 74:86-92, 2012.   \n[46] Trick, L. M., J. T. Enns. Lifespan changes in attention: The visual search task. Cognitive Development, 13(3):369-386, 1998.   \n[47] Wolfe, J. M., E. M. Palmer, T. S. Horowitz. Reaction time distributions constrain models of visual search. Vision research, 50(14):1304-1311, 2010.   \n[48] Zhang, D., G. Lu. Evaluation of similarity measurement for image retrieval. In International conference on neural networks and signal processing, 2003. proceedings of the 2003, vol. 2, pages 928-931. IEEE, 2003.   \n[49] Wang, J., Y. Song, T. Leung, et al. Learning fine-grained image similarity with deep ranking. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1386-1393. 2014.   \n[50] Tudor Ionescu, R., B. Alexe, M. Leordeanu, et al. How hard can it be? estimating the difficulty of visual search in an image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2157-2166. 2016.   \n[51] Cao, B. B., L. O'Gorman, M. Coss, et al. Data-side efficiencies for lightweight convolutional neural networks. arXiv preprint arXiv:2308.13057, 2023.   \n[52] Radford, A., J. W. Kim, C. Hallacy, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021.   \n[53] Oquab, M., T. Darcet, T. Moutakanni, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.   \n[54] Kumar, M., N. Houlsby, N. Kalchbrenner, et al. Do better imagenet classifiers assess perceptual similarity better? arXiv preprint arXiv:2203.04946, 2022.   \n[55] Contributors, M. Openmmlab's pre-training toolbox and benchmark. https : //github. com/ open-mmlab/mmpretrain, 2023.   \n[56] He, K., X. Zhang, S. Ren, et al. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778. 2016.   \n[57] Simonyan, K., A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.   \n[58] Woo, S., S. Debnath, R. Hu, et al. Convnext v2: Co-designing and scaling convnets with masked autoencoders. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16133-16142. 2023.   \n[59] Szegedy, C., V. Vanhoucke, S. Ioffe, et al. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818-2826. 2016.   \n[60] Dosovitskiy, A., L. Beyer, A. Kolesnikov, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n\n435 [61] Liu, Z., H. Hu, Y. Lin, et al. Swin transformer v2: Scaling up capacity and resolution. In   \n436 Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages   \n437 12009-12019. 2022.   \n438 [62] Stallkamp, J., M. Schlipsing, J. Salmen, et al. Man vs. computer: Benchmarking machine   \n439 learning algorithms for traffic sign recognition. Neural Networks, (0):-, 2012.   \n440 [63] Krizhevsky, A., G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.   \n441 [64] Bossard, L., M. Guillaumin, L. Van Gool. Food-101 - mining discriminative components with   \n442 random forests. In European Conference on Computer Vision. 2014.   \n443 [65] Li, F.-F., M. Andreeto, M. Ranzato, et al. Caltech 101, 2022.   \n444 [66] Griffin, G., A. Holub, P. Perona. Caltech 256, 2022.   \n445 [67] Ha, D., D. Eck. A neural representation of sketch drawings. arXiv preprint arXiv:1704.03477,   \n446 2017.   \n447 [68] Wah, C., S. Branson, P. Welinder, et al. The Caltech-UCsD Birds-200-201l Dataset. 2011.   \n448 [69] Quattoni, A., A. Torralba. Recognizing indoor scenes. In 2009 IEEE conference on computer   \n449 vision and pattern recognition, pages 413-420. IEEE, 2009.   \n450 [70] Cimpoi, M., S. Maji, I. Kokkinos, et al. Describing textures in the wild. In Proceedings of the   \n451 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 2014.   \n452 [71] Foundation, T. L. Pytorch hub. https://pytorch. org/hub, 2024. Accessed on 2024-06-04.   \n453 [72] Inc., G. Tensorflow hub. https://www.tensorf1ow. org/hub, 2024. Accessed on 2024-06-   \n454 04.   \n455 [73] Face, H. Hugging face models. https://huggingface.co/models, 2024. Accessed on   \n456 2024-06-04.   \n457 [74] Geirhos, R., D. H. Janssen, H. H. Sch\"utt, et al. Comparing deep neural networks against   \n458 humans: object recognition when the signal gets weaker. arXiv preprint arXiv:1706.06969,   \n459 2017.   \n460 [75] Rajalingham, R., E. B. Issa, P. Bashivan, et al. Large-scale, high-resolution comparison of the   \n461 core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial   \n462 neural networks. Journal of Neuroscience, 38(33):7255-7269, 2018.   \n463 [76] Shahapure, K. R., C. Nicholas. Cluster quality analysis using silhouette score. In 2020 IEEE   \n464 7th international conference on data science and advanced analytics (DSAA), pages 747-748.   \n465 IEEE, 2020.   \n466 [77] Wicklin, R. Weak or strong? how to interpret a spearman or kendall correlation. https:   \n467 //blogs.sas.com/content/im1/2023/04/05/interpret-spearman-kendal1-corr.   \n468 htm1, 2024. Accessed on 2024-06-04.   \n469 [78] Schober, P., C. Boer, L. A. Schwarte. Correlation coefficients: appropriate use and interpretation.   \n470 Anesthesia & analgesia, 126(5):1763-1768, 2018.",
  "status": "success",
  "markdown_file": "full.md"
}